Lawmakers are looking for Affordable Care Act savings in the wrong place. Removing sick people from risk pools or reducing health plan benefits〞the focus of lawmakers＊ attention〞would harm vulnerable populations. Instead, reform should target the $210 billion worth of unnecessary care prescribed by doctors, consented to by patients, and paid for by insurers.

This Article unravels the mystery of why the insurance market has failed to excise this waste on its own. A toxic combination of mismatched legal incentives, market failures, and industry norms means that the insurance market cannot solve the problem absent intervention.

But this intervention could be a simple nudge: steering decisionmakers away from unnecessary care, while protecting the autonomy of doctors and patients. Insurers should require, by contract, that providers receive an automated warning before ordering commonly overused interventions. Such computer-driven nudges have been effective in other contexts and would reduce premiums without harming those most in need of help. Because insurers lack appropriate incentives to nudge, the law must mandate them.

In the current health reform debate, policymakers are looking for ways to lower both premiums and overall health insurance costs. Discussions have largely centered on options like reducing coverage or removing sick people from risk pools.[1] These approaches, however, would severely harm vulnerable segments of the population.[2] Policymakers should instead focus on ways to reduce unnecessary care.

Health care spending in the United States is bloated by payments for expensive treatments that are unnecessary and ineffective.[3] Arthroscopic knee surgery for osteoarthritis works no better than a placebo surgery, and yet it continues to be routinely performed.[4] Pre-operative chest X-rays are often ordered for patients with no symptoms of heart or lung disease, even though they are unlikely to yield useful information.[5] MRI＊s for uncomplicated headaches are on the rise, despite guidelines recommending they not be used.[6] And the list goes on.[7]

The United States significantly outspends all other industrialized nations in health care, yet does worse in most measures of quality.[8] Unnecessary care is a big part of the problem.[9] By some estimates, spending on unnecessary care accounts for as much as 30% of total health care spending〞on the order of $750 billion per year, with $210 billion of that spent on unnecessary services.[10] An Institute of Medicine report made the sobering analogy that if the price of milk had grown as quickly since 1945 as the cost of unnecessary health care, ※a gallon of milk would [now] cost $48.§[11] Spending on unnecessary care raises systemic costs, ultimately raising insurance premiums and pricing some patients out of the market. It can also harm patients who are exposed to unnecessary radiation, complications, infections, and emotional harm.[12]

Unnecessary care is consumed because doctors prescribe it, patients consent to it, and payors pay for it. The role of doctors and patients in creating this problem has received considerable attention in the literature.[13] Economic incentives, cultural norms, and legal incentives stemming from the medical malpractice system cause doctors to over-test, over-prescribe, and over-treat.[14] Patient decision-making is also subject to informational deficits and cognitive biases that lead many to err on the side of more treatment〞doing something feels better than doing nothing.[15] Furthermore, moral hazard makes patients inefficiently price sensitive,[16] and lack of price transparency impedes patient ability to take cost into account in making decisions.[17] Significant efforts are underway to address these patient and physician-centric problems, although they are not without challenges.[18]

But what is particularly perplexing is why insurance companies and government payors are reimbursing for high-cost, unnecessary care. Of all the actors in the complicated web that is health care, insurance companies would seem best situated to act as a check on patients and doctors. A rational, profit-maximizing insurance company〞or even a nonprofit one〞should be motivated to refuse reimbursement for unnecessary care that fails to improve patient health and might even harm it. Insurers able to reduce claims costs would increase profit margins or at least be able to reduce premiums. But while insurers occasionally refuse coverage, particularly for so-called ※experimental§ procedures, the vast majority of the time, they defer decisions on the medical necessity of treatment to physicians.[19]

There are reasons that insurers do so, grounded in norms, imperfect markets, and laws.[20] For one, it can be hard to identify with requisite certainty which expensive procedures are likely to be ineffective.[21] And the possibility of error in these determinations could have dire consequences. Consider, for example, the insurer that refuses to pay for a certain cancer treatment that, it later learns, would have saved the patient＊s life. Private insurers, for a variety of reasons, tend to follow the lead of Medicare in coverage decisions.[22] Insurers also might lack adequate incentives to incur the administrative costs of a stricter medical necessity review, particularly when insurers can instead raise premiums in an imperfectly competitive market.[23] And in some cases, state statutes and common law require insurers to defer to doctors on the medical necessity of care.[24]

But perhaps the biggest hurdle is the norm of patient and physician autonomy. Health Maintenance Organizations (※HMOs§) were strongly criticized when their utilization reviews were said to infringe on patient and physician autonomy in making medical decisions.[25] The predominant arguments were that each patient is different, medicine is more of an art than a science, and doctors employed by insurance companies should not be able to override the decision of a doctor who has actually examined the patient.[26] The autonomy argument is still commonly made today〞often in the form that a patient＊s right to self-determination is particularly strong in health care where patients are charged with making very personal decisions that affect their own bodies.[27]

But while patient and physician autonomy are important, patients and physicians do not operate in a vacuum. When patients and physicians choose unnecessary care, and that care is reimbursed by a private or government payor, the decision has repercussions beyond the patient＊s individual well-being. Other members of the risk pool bear the reimbursement cost.[28]

And there are other considerations. Since the original outcry against HMO utilization reviews, data has improved. While there are circumstances in which the individual patient＊s knowledge is particularly important, much can be learned from the data that apply across broad categories. We can now identify some common tests and procedures that are over-used and the circumstances in which such overuse is particularly likely. Over time, we will be able to identify more circumstances and make these determinations more personalized.[29] There are many organizations〞private and governmental〞gathering this data.[30] And while there is still a tremendous amount of work to be done, all indications are that progress will continue.

Also, the challenge of addressing overuse of health services by changing patient and doctor behavior persists. De-biasing decisionmakers is difficult, transparency is slow in the making, and changing payment incentives can have perverse effects.[31] Consider, as well, that we now better appreciate how slow medical practice is to adapt to new evidence. One study found that it takes, on average, seventeen years for custom and practice in medicine to catch up with the evidence, meaning that the majority of doctors continue to adhere to out每of每date practices long after the evidence mandates change.[32] Custom and practice-based medical malpractice law, therefore, also does little to stem the tide of unnecessary care.[33]

The time has therefore come to revisit the role of payors in addressing the unnecessary care problem. Payors cannot be tasked with making reimbursement decisions absent input from physicians. There are too many land mines, including that insurer financial motivations can prompt denial of reimbursement for profit maximization reasons, rather than for the betterment of patient health.[34] Yet individual circumstances and doctor intuition still matter. Political headwinds against ※death panels§ are too strong to contemplate payor power absent a role for doctors and patients. Neither can unabated physician and patient autonomy in decision-making carry the day. Payment decisions not only affect individuals, but impose costs on society writ large.

The problem begs for a middle path〞a way to steer decisionmakers away from unnecessary care while still protecting the right to choose.[35] Insurers motivated by reducing claims costs should require the implementation of a nudge: an automated warning to providers when they try to order a test that is known to be overused or a treatment that evidence suggests will be ineffective. Essentially, the insurer should require the provider＊s ordering system prompt the doctor with a warning saying, ※are you sure you want to prescribe this test?§ There is preliminary evidence that such nudges can move the needle in significant ways.

There are hurdles to consider. Caution should be exercised about the quality of the data on which nudges are based. Building the necessary information technology (※IT§) capacity would be costly. Providers might become conditioned to these warnings and begin to ignore them or might resent the additional administrative step of having to click through another screen. But perhaps the most important hurdle is that payors are inadequately incentivized to implement these nudges on their own. Regulators therefore need to mandate such a system.

Part I of this Article starts by describing the problem of unnecessary care, the actors who cause it, even if inadvertently, and its consequences. Attempts to address the problem by changing patient and physician behaviors are well-intended but so far have not yielded enough positive change. Notably, far too little attention has been paid to payors＊ roles in the unnecessary care problem.

Part II takes up the issue of payors specifically and addresses why they now do little to screen for unnecessary care. It explores the evolution in payor approaches to coverage determinations, from complete respect for physician autonomy to more active insurer decision-making and back again, and concludes by exploring payor incentives. A combination of imperfect markets, laws backing physician autonomy, and industry norms explain why the market has not corrected the unnecessary care problem on its own.

Part III makes the normative argument that unalloyed respect for physician and patient autonomy is dangerous. On the other hand, failure to guard autonomy entirely is equally problematic. Such situations are ripe for nudges. In order to move decision-making away from the problematic poles between which the industry has vacillated over its history, and given the imperfect functioning of insurance markets, this Part suggests that a nudge may be required.

Finally, Part IV sets out the proposed solution〞an automated warning to providers when they try to order care that is likely to be unnecessary. Studies of warning-type nudges give reason for optimism that this transparent and autonomy-respecting regime can be successful, although Part IV also explores the challenges to its success.

I. The Problem: High Rates of Unnecessary and Ineffective Health Care

The Affordable Care Act (※ACA§) has been shrouded in controversy〞both political and legal〞since its passage in 2010. Heated debates about the individual mandate,[36] Medicaid expansion,[37] and religious objections to coverage of contraceptives[38] have dominated the conversation. Although these debates continue, the rhetoric on repealing and replacing the ACA has centered of late on a very practical problem: high premium rates.

A. Rising Premium Rates in Health Care and the Unnecessary Care Cause

Premiums rates have risen significantly over the past decade.[39] Rising premiums were an important theme in the 2016 presidential election. Then-candidate Donald Trump pressed his case for doing away with the ACA with particular fervor following the release of a government report in October 2016 that detailed rate hikes. In one of the presidential debates, he stated: ※[O]ne thing we have to do, repeal and replace the disaster known as Obamacare. [I]t＊s destroying our country . . . . [T]he premiums are going up 60, 70, 80%.§[40] The issue of high premium rates was, by many accounts, salient with voters.[41]

After the election, President Trump and other Republican leaders continued to press the point, emphasizing the need to lower premiums as a key reason to repeal and replace the ACA.[42] In fact, premiums do generally seem to be on the rise, although increases have been highly variable nationwide.[43] On average, premiums for individual policies purchased on the Exchanges rose about 25% in 2017.[44] For those who obtain their policies through employers, premiums increased by a more modest amount, but still went up faster than wages.[45]

There is some debate about just how problematic increasing premium rates are. The premium increases largely track the 2009 Congressional Budget Office estimates for premium rates at the time the ACA was passed.[46] And despite increasing premiums, about 85% of enrollees through the Exchanges receive premium tax credits, which lower their premiums to 10% of their income.[47] Even so, someone has to pay when premium rates go up.[48] For the most part, that health care costs are too high and that health reform should focus on bringing down costs generally and premiums specifically are rare points on which liberals and conservatives in the health care debate agree〞even if they disagree on how to go about it.[49]

There are many reasons why health insurance premiums have gone up under the ACA.[50] Recent attention, particularly among Republicans, has focused on two such reasons: the requirement of more robust coverage and the expense in covering sicker insureds.[51] The ACA requires that plans cover ※essential health benefits,§ which include ten core categories of health care services.[52] Plans providing broader coverage are raising premiums to cover the additional claims costs.[53] The ACA also prevents plans from experience rating, which means charging sicker patients higher rates. Because plans are prohibited from excluding patients with pre-existing conditions and cannot charge them more, premiums have also gone up.[54]

But fixing either of these problems is difficult and requires making troubling trade-offs. For instance, Republican plans to replace the ACA have contemplated allowing insurers to offer skimpier coverage or permitting insurers to charge sicker people higher premiums.[55] Whether these results are palatable turns on difficult value judgments, but for many, these approaches are inscrutable and impossible to justify because of their negative impact on vulnerable populations like the sick and the poor.[56]

Yet there is another important contributor to high premium rates that is well-documented, but has largely eluded policymaker focus in the repeal and replace debate: the fact that premiums are hugely bloated by unnecessary care.[57] Fixing this problem would implicate far fewer value judgments. If it were possible to reduce premiums by cutting back on unnecessary care, it would be hard to imagine many objections.[58] And policymakers might not have to submit to skimping on benefits or failing to cover sick people to decrease premiums. The next section describes the problem of unnecessary care and how it impacts insurance rates.

The term ※unnecessary health care§[59] describes the provision of ※services which show no demonstrable benefit to patients.§[60] Unnecessary care includes both ※overutilization§〞too much care that does not improve patient health[61]〞and ※misconsumption§〞the wrong choice of care when a different choice (or even doing nothing) would lead to better outcomes.[62] Taken together, overutilization and misconsumption have dire consequences for both the cost of health care and patient well-being.

There are many examples of overutilization of care, but perhaps the most prominent are the overuse of imaging, diagnostic tests, and antimicrobials (including antibiotics).[63]

Imaging studies, such as magnetic resonance imaging (※MRI§), ultrasound imaging, computed tomography (※CT§) scans, and conventional X-rays, create visual representations of the body＊s inside for clinical analysis.[64] Advances in imaging techniques have greatly enhanced the ability of physicians to diagnose a wide variety of ailments.[65] But imaging can be both expensive[66] and harmful to patients, and many studies have now determined imaging to be markedly overused.[67] For instance, X-rays for uncomplicated lower back pain, CT scans for sinusitis, and MRI scans for uncomplicated headaches, are all commonly performed but often not clinically indicated.[68] A 2016 University of Michigan study found that nearly 60% of the advanced imaging ※performed for more than 29,000 Michigan women diagnosed with early breast cancer between 2008 and 2014 could not be medically justified based on retrospective record review.§[69] One report estimates that 20每50% of all high-tech imaging is unnecessary.[70]

Laboratory tests are similarly overused〞by some estimates on the order of 60每70% more than necessary.[71] These tests do not contribute towards management of patients.[72] Consider as a particularly illuminating example the documented overuse of BRCA-1 genetic tests in patients for whom such testing was not clinically indicated.[73] Although the BRCA-1 tests have come down in price following the Supreme Court＊s holding in Association for Molecular Pathology v. Myriad Genetics that isolated DNA sequences are not patentable,[74] the Myriad test at one point cost $3,000.[75]

Finally, the prescription of antimicrobials (which includes antibiotics and antifungals) when medically inappropriate is an important example of overutilization.[76] In 2016, the Centers for Disease Control and Prevention released a study in collaboration with other medical experts that found that at least 30% of antibiotics prescribed in the United States are unnecessary.[77] Overuse includes the prescription of antibiotics for conditions caused by viruses that do not respond to antibiotics.[78]

In addition to overutilization, much care is also misconsumed in the United States. Misconsumption is essentially inappropriate treatment that medical guidelines do not support, but that doctors nonetheless perform.[79] Misconsumption often comes from treatments that became popular absent evidence of efficacy〞and maybe even became the standard of care〞but were later determined to generally not be effective. While some misconsumption naturally ends over time once evidence of ineffectiveness becomes widely known, much still persists. Often misconsumption continues for many years after dispositive evidence that the procedure is ineffective.[80] In other words, the standard of care is very slow to conform to evidence.[81]

Consider the case of knee arthroscopy for osteoarthritis, where a fiber每optic endoscope and surgical instruments are inserted into the knee to smooth rough surfaces and repair tears in the cartilage and meniscus.[82] A review of past gold standard studies clearly shows that the procedure is not beneficial to osteoarthritis patients, yet doctors continue to perform it.[83]

There are many other procedures that fall in a similar category: spinal fusion surgeries for low back pain on worn out discs,[84] vertebroplasty for osteoporotic vertebral fractures,[85] placement of coronary stents in patients with nonacute indications,[86] laparoscopic uterine nerve ablation for chronic pelvic pain,[87] and removal of healthy ovaries during a hysterectomy.[88] Radical mastectomy was the standard of care for breast cancer for decades and continues to be in wide use even after a randomized trial revealed that it was no better at protecting women from cancer than more conservative techniques.[89]

Rates of both overutilization and misconsumption in the United States are disturbingly high. The United States spends approximately 17% of gross domestic product (※GDP§) on health care services〞the highest percentage by a wide margin of any industrialized nation.[90] According to the Institute of Medicine, the provision of unnecessary care accounted for more than 8% of health care spending in the United States in 2009 ($210 billion out of $2.6 trillion).[91] The Congressional Budget Office warns that the costs associated with unnecessary care are significant and growing.[92] Studies of Medicare specifically have led to similar conclusions. One well-known study of Medicare claims found large regional variations in Medicare spending, with enrollees in higher每spending regions receiving more care than those in lower每spending regions.[93] Yet health outcomes and satisfaction with care were no greater in the higher utilization regions than in the low utilization regions.[94]

As National Institutes of Health bioethicist Ezekiel Emanuel and Stanford economist Victor Fuchs concluded in their 2008 article, The Perfect Storm of Overutilization, unnecessary care[95] is the most important contributor to high health care costs in the United States.[96] This cost is problematic both because of the financial impact and the potential harm unnecessary care causes patients.

First, consider basic economics. When insured patients receive unnecessary care, they generate claims costs that must be paid by the insurer. Insurers, and the actuaries that work for them, analyze claims costs in setting premium rates. When claims costs increase, insurers respond by raising premiums to cover the costs.[97] As premiums increase, poorer insureds in the individual market who pay premiums out of pocket become uninsured as they can no longer afford the premiums.[98] Poor, uninsured individuals who do not qualify for Medicaid cannot access care except through charity and in emergency rooms.[99] That care becomes more costly than before the care became emergent.[100] And it likely results in debt, which is recovered only through government funds or hospitals raising overall rates.[101] The bottom line is that increasing premiums has a domino effect that not only negatively impacts individuals who have to pay more to cover insurance premiums, but also increases the number of uninsureds with all of the consequences that follow.

The Affordable Care Act attempted to address the problem by subsidizing premiums for poorer individuals through tax credits.[102] As premiums increase, individuals receive larger subsidies so that the percentage of their income they must pay to cover premiums is fixed. But while the tax credits may indeed prevent people from becoming uninsured, rising premium rates means the government must pay ever-increasing rates to subsidize insurance for these individuals.[103]

High rates of unnecessary care also result in premium increases for employer group insurance plans.[104] In response to increasing premiums, employers must either pass a higher percentage of costs onto employees[105] or they must make other sacrifices to try to reduce costs, such as by narrowing networks, increasing deductibles and copays, or choosing less robust coverage plans. Alternately, employers may pay employees less to account for larger employer shares of health insurance premiums.[106]

And it is not just private insurance that is affected. High rates of unnecessary care increase government spending under Medicare and Medicaid, too. The Congressional Budget Office has found that higher per beneficiary spending spurred by unnecessary testing and procedures is a clear driver of rising Medicare expenses.[107]

Second, patients are harmed in non-economic ways by unnecessary care. The delivery of care inherently involves risk and may lead to complications. Patients who needlessly receive medical interventions are subject to ※health care-associated infections, . . . post-operative complications such as blood clots,§ and other harms.[108] A recent report suggests that medical errors might be the third most common cause of death in the United States.[109] Unnecessary care increases a patient＊s risk of being subject to medical errors.

Further, exposure to excess radiation from unnecessary imaging can be dangerous. One study estimates that excess radiation from overuse of CT and MRI scans causes 1.5%每2% of all cancers.[110] Unnecessary diagnostic testing can also lead to overdiagnosis〞that is, the diagnosis of a person with a condition that will not cause harm or would otherwise have remained irrelevant.[111]

Problems from overuse of antimicrobials are also significant, putting patients at needless risk for allergic reactions and other side effects. The overuse of antibiotics drives antibiotic resistance, endangering patients who actually require antibiotics to treat bacterial infections.[112]

Given this parade of horribles, the logical question is how we have ended up with such tremendous levels of unnecessary care being consumed in American health care.

C. The Actors That Contribute to High Rates of Unnecessary Care

High rates of unnecessary care are not attributable to a single actor. Most of the scholarly focus to date has been on the contribution of patients and doctors to the problem. In their famous 2008 article, Drs. Emanuel and Fuchs described how doctor and patient incentives lead to the ※perfect storm of ＆more＊§ care.[113]

Doctors＊ contributions to unnecessary care are well-documented in the literature and therefore only briefly described here. They include the fee-for-service reimbursement system, the perceived need to practice defensive medicine, and the training and culture of physician-delivered care.

First, doctors are typically paid based on the services they provide. The more tests and procedures ordered and performed, the higher their compensation.[114] A doctor therefore has a financial incentive to do more, not less.

Second, the medical malpractice liability regime encourages doctors to practice ※defensive medicine,§ where doctors order more diagnostic tests and perform more medical procedures to protect them from later being sued for not doing enough.[115] In a Pennsylvania study, 92% of the 824 physicians surveyed reported ordering imaging tests and diagnostic measures for assurance against malpractice liability.[116]

Finally, cultural norms spur unnecessary care. Doctors are instilled with a commitment to leave no stone unturned on behalf of their patients and to put patient health above financial considerations.[117] At the same time, there are no approved standards of care. What physicians learn in medical school becomes their standard practice, and it can be difficult to uproot learned practices despite new evidence.[118] It can also be difficult for physicians to keep abreast of new evidence.[119] Further, incentive to conduct high quality studies of effectiveness is lacking given that payment is based on standard practice, not evidence, and there is typically little opportunity to monetize research findings.[120]

Given how slow medical practice is to adapt to new evidence,[121] custom and practice-based medical malpractice law also does little to stem the tide of unnecessary care. The FDA regulates pharmaceuticals for efficacy, if imperfectly,[122] but only the laws of medical malpractice and informed consent〞and the market if it were properly functioning〞serve as a check on the provision of ineffective medical services.

Attempts have been made, and continue to be made, to mitigate these problems. Payment reforms attempt to encourage doctors to reduce costs while maintaining quality〞to give doctors an incentive to avoid low-value care.[123] And tort reform attempts to lessen the perceived need to practice defensive medicine.[124] But these solutions, at least as of yet, have not significantly stemmed the tide of unnecessary care.[125]

Doctors are not alone in contributing to the unnecessary care problem. Patients must consent to the care they receive.[126] While some patients are merely susceptible to their physician＊s suggestions, others drive the provision of unnecessary care, convincing their doctors to agree to their desired care.[127]

For one, patients are subject to a number of cognitive biases that cause them to insist on extra tests or prescriptions.[128] There is a cultural preference for ※more,§ particularly when it comes to technological solutions.[129]

Adding to this, the nature of insurance creates moral hazard, where patients lack adequate financial incentive to refuse unnecessary care.[130] Patients＊ monthly premiums are a sunk cost, and they spend little out-of-pocket (at least historically) for the care they consume.[131]

Lack of price transparency further exacerbates the situation.[132] Patients who might be deterred from overuse by the cost of tests and procedures are not motivated to turn down care because they do not have insight into cost at the time of decision-making.

As with physician incentives, efforts have been made to address patient incentives〞to improve patient decision-making so that it does not contribute to waste of health care resources. For instance, there has been a big push in the last decade to de-bias patient decision-making and encourage rational choice.[133] Efforts have been made to address moral hazard by giving patients more skin in the game through consumer-driven health care. As a supplement to these efforts, policymakers have tried to make costs more transparent. These policy initiatives attempt to stem the tide of unnecessary care.[134] But just as with efforts to address physician biases, these patient-centric efforts have, so far, not made a big impact.

What is obviously missing from this story is a discussion of payors who currently reimburse the unnecessary care that is being consumed. The next Part explores why payors tend to be complicit in the storm of unnecessary care, despite what would appear to be contrary incentives.

II. Rational Insurers Should Screen for Unnecessary Care, But Laws, Markets, and Norms Get in the Way

While physician and patient incentives to increase unnecessary care are well-documented, it is perplexing why insurance companies[135] are paying for high-cost, unnecessary care. Of all of the actors in the complicated health care web, insurance companies seem best situated to act as a check on patients and doctors. A rational, profit-maximizing insurance company〞or even a nonprofit one[136]〞should be motivated to refuse reimbursement for unnecessary care to lower claims costs.

This Part details how insurance companies have approached coverage determinations over time. It then addresses the various counterintuitive reasons〞grounded in market failures, legal incentives, and industry norms〞that insurers today often reimburse for unnecessary care.

Private health insurance is governed by what essentially amounts to a three-party contract. Patients agree to pay insurance companies fixed monthly premiums, and these premiums entitle patients to coverage, which means the insurer makes payments to cover the patient＊s care. Providers are reimbursed for the care they provide (either directly or indirectly) by these insurer payments.[137]

Insurance contracts are necessarily incomplete.[138] With perfect information, an insurer could specify by contract what it is agreeing to reimburse for. But typically, the parties do not know at the time of contracting what type of care the patient will need or what an appropriate treatment for the patient will be.[139] Insurance contracts address this problem in two general ways. First, they define broad categories of care that are either covered or excluded.[140] For instance, emergency room visits might be covered, but experimental treatments or cosmetic procedures excluded. Second, within those categories for which a policy provides coverage, the insurer agrees to reimburse only for tests and procedures deemed ※medically necessary.§[141] This approach to defining coverage is intended as a work-around for the incomplete contracts problem.[142]

Over time, approaches to determining the bounds of reimbursement have evolved, from an early period marked by deference to physician decision-making on matters of medical necessity, to insurer attempts to more actively manage the reimbursement process, and then back again to a mostly physician-centered approach.[143]

Although the term ※medically necessary§ is vague, it initially mattered little. Prior to the mid-twentieth century, medicine was dominated by physician paternalism.[144] Physicians made medical decisions on behalf of patients, and insurance companies reimbursed for the care provided without question.[145] Use of the term ※medical necessity§ in contracts was really a delegation to one party〞the doctor〞to make decisions about what care was required.

The few court cases that emerged from this period are consistent with the norm of deference to physician decision-making. For instance, in the 1966 case Mount Sinai Hospital v. Zorek, a patient＊s physician sought to hospitalize a patient to treat her obesity.[146] In a move that was rare for the time, the insurer objected to paying for the hospitalization.[147] The court concluded that the insurer must defer to the treating physician and pay for the treatment prescribed.[148] The court reasoned that ※[o]nly the treating physician can determine what the appropriate treatment should be for any given condition. Any other standard would involve intolerable second-guessing . . . .§[149]

However, by the early 1970s, the tide started to turn, as the United States faced the first real crisis of rapidly expanding health care costs.[150] With doctors paid on a fee-for-service basis, and patients largely insulated from cost by their insurers, demand and reimbursement for services rose quickly.[151] In response, policymakers turned to managed care to contain costs.[152] With it came new approaches to insurer reimbursement that were less deferential to physicians.

Although managed care was first introduced after World War I, it gained popularity as a delivery model in the 1970s and 1980s. Managed care organizations (※MCOs§) were designed primarily to manage cost and utilization.[153] Their ascendancy eroded physician power in driving reimbursement decisions.[154]

One of the key ways that MCOs attempted to rein in costs was by more stringently reviewing utilization of health care resources〞by both prospective and retrospective review of claims.[155] Prospective review required physicians to justify the medical necessity of the prescribed service or procedure to the insurer before the service was delivered.[156] It usually took the form of requiring pre-approval or prior authorization.[157] The idea was to weed out unnecessary care before it occurred.

With retrospective review of claims, insurers reviewed requests for reimbursement after care had already been delivered, refusing requests the insurer deemed unnecessary.[158] The purpose of retrospective review was to uncover provider practice problems and deter physicians from delivering high-cost, unnecessary care.[159]

The overall result was that insurers during this period routinely refused to reimburse for care they deemed unnecessary. Particularly for patients covered by MCOs, the provision of health care services was heavily influenced by insurer decisions, not just clinical decisions.[160]

Physicians and patients did not accept this new reality without resistance. Physicians abhorred the administrative burdens MCOs imposed.[161] And they argued that insurers should not be able to substitute their judgment for a physician＊s expertise. They noted that each patient has individual needs and circumstances that cannot be properly appreciated by an insurance company.[162] Much of medicine is based on intuition, not hard science, hence there was often no legitimate basis on which insurers could make decisions.[163] Physicians also criticized utilization review because it lacked transparency.[164] Patients were similarly angered, primarily by their inability to obtain the care that their doctors told them they needed.[165]

But perhaps the strongest argument against insurer-driven reimbursement decisions was that insurers had a perverse incentive to ration care.[166] Motivated by controlling costs and generating profits, insurers＊ reasons for declining coverage, commonly believed to be based on cost containment and not medical appropriateness, were inherently suspect.[167]

Courts prevented insurers from entirely taking the reins in decision-making during this period, continuing in many cases to define medical necessity with reference to physician judgment.[168] But by most accounts, it was the political failure of insurer-driven reimbursement decisions that doomed the approach.[169] By the late 1990s, the pendulum had swung back, at least mostly, to deference to physician decision-making.

3. The Pendulum Swings (Mostly) Back to Physician-Driven Reimbursement (1990s每Present)

Today, private (and public) insurers are mostly punting to doctors on questions of medical necessity.[170] Insurers do still have processes for deciding whether to cover new technologies.[171] And most readers have probably had some experience with insurers refusing to pay or requiring that certain steps be taken before reimbursing for treatment. But it is not common for insurers (or the government) to evaluate the effectiveness of interventions that are already in common use〞even if sound evidence suggests that the intervention is ineffective.[172] Without overwhelming evidence of harm, it is uncommon for an insurer to refuse reimbursement.[173]

The reasons for this are complicated. In response to the backlash against utilization review, laws have made it more difficult for insurers to take on physician decision-making, administrative processes have been put in place to make it easier to challenge insurer decision-making, and other realities have resulted in insurers taking a back seat to physicians and patients in assessing the necessity of care for reimbursement purposes.

B. Why Payors Often Reimburse for Unnecessary Care

Rational insurers should be motivated to screen and refuse to pay for unnecessary care. Why don＊t they? This section explores the legal, cultural, and market-based reasons.

It can be difficult and therefore costly for insurers to administratively identify unnecessary care. Unlike with pharmaceuticals,[174] no agency approves the effectiveness of medical procedures before they become the standard of care. Nor does any agency review continued effectiveness.[175] One of the major arguments against evidence-based medicine has always been that much of medical practice is not based strictly in science.[176]

Relatedly, differences in individual patients, detectable by the treating physician, can be hard for an insurance company to identify. These differences may make care necessary for one patient, but not another.

For these reasons, ※putting a great deal of effort into identifying and eliminating unnecessary care is likely to increase rather than decrease administrative costs.§[177] If administrative costs in identifying unnecessary care are higher than insurers＊ savings in rejecting reimbursement for unnecessary care, that would explain why insurers now mostly defer to physician decision-making.[178]

Given this background, perhaps it is not surprising that there is a strong industry norm of private insurers simply following Medicare＊s coverage decisions.[179] The advantage is that Medicare bears the administrative cost of determining when care should be reimbursed and when it should not.[180] The disadvantage〞which is a significant one〞is that Medicare＊s process is flawed and not designed to identify unnecessary care.[181] Rather, Medicare tends to consider whether to cover new technologies and procedures, infrequently revisiting decisions already made, despite developments in the evidence of effectiveness. Or at least, Medicare is very slow to update its coverage decisions.[182]

Following Medicare decisions, however, also has the advantage〞at least perceived to be so by private insurers〞of insulating them from liability.[183] The next Section discusses the legal regime that deters insurers from more actively policing unnecessary care.

There are a number of legal constraints that insurers face in reviewing patient and physician decision-making about care. First, states responded to the political failure of utilization review by building up a large statutory and regulatory apparatus to monitor the actions of insurance companies. Some states require plans to use a specified definition of medical necessity and require insurers to defer to the judgment of physicians.[184] For example, in Louisiana, medical necessity is defined to include ※health care services . . . that are considered by most physicians . . . to be the standard of care.§[185] In states that do not mandate a definition of medical necessity, many still require plans to submit their proposed definitions to state regulators for approval.[186]

Other restrictions on insurers are also in common use. The majority of states restrict insurer use of pre-authorization procedures[187] and the time that plans take when making prospective medical necessity determinations[188] and dictate the information that must be provided in denial letters.[189]

Perhaps the most impactful change, though, has been the increase in legislatively-mandated review processes, both internal and external, for medical necessity denials. These processes are intended to give insureds better recourse.[190] Internal reviews generally involve review by a plan physician who was not involved in the initial denial and then review by an internal committee.[191] Once internal processes are exhausted, insureds may be entitled to external review.[192]

This growth in regulation was based on the need to address real abuses of the utilization review process, as described above. But it also impacts insurers＊ abilities to screen for unnecessary care and adds significant cost to insurers that refuse reimbursement for unnecessary care if their decisions are challenged.

Second, insurers are also deterred from more aggressively screening for unnecessary care by the cost of litigation. When an insurer rejects a reimbursement request on the basis that the care is not medically necessary, it can be sued for breach of contract. Then, in subsequent litigation, insurers face hurdles. Many courts still interpret ※medical necessity§ to be congruent with physician judgment and standard of care (and not necessarily evidence).[193] Also, ambiguities in insurance contracts are construed in favor of the insured.[194] The conventional wisdom is that courts tend to be sympathetic to plaintiffs seeking access to care they would not be able to afford absent coverage.[195] Indeed, there have been a number of high profile cases where insurers have been ordered to pay for care even where evidence of effectiveness was lacking.[196]

More recently, some scholars have expressed skepticism that medical necessity litigation is as one-sided in favor of insureds as previously thought.[197] But regardless of insurer success in these lawsuits, they provide an additional cost that deters insurers from attempting to screen for unnecessary care.

Insurers may face other liability, as well. In addition to contract suits, insurers can be sued in tort for bad faith performance of their duties to insureds.[198] And insurers have been sued〞and held liable〞under state insurance law.[199]

Even if insurers win when they are sued, they incur costs〞both financial and often reputational, as well. ※[I]nsurers are acutely aware that a well-publicized dispute over an inappropriately denied claim might cause them to lose the next renewal of their contract.§[200]

For all of these reasons, insurers are deterred from a more aggressive review of unnecessary care. But there is yet another consideration, which is that insurers have other, more foolproof ways of generating profits.

Maximizing profits[201] requires either reducing costs or increasing revenues (or even better, doing both). Where reducing costs is difficult, insurers may turn to the other option. Notably, they can increase revenues by raising premiums. Insurers frequently choose this option. Premiums have been steadily increasing over the last decade, while costs for unnecessary care have been on the rise.[202]

There should be both regulatory and market-driven checks on insurers＊ abilities to raise premiums. Neither, however, seem to have significantly stemmed the tide of rate increases.

First, a competitive market should constrain insurer ability to raise premiums. Theoretically, if insurers raise rates higher than competitors, consumers would purchase plans from the competitors and revenues might actually decrease rather than increase.[203] Although advocates of the private health insurance system often assume that competition will drive down prices, empirical evidence supporting this theory is scant.[204] Rather, there is a growing body of evidence that health insurance markets are not perfectly〞or even adequately〞competitive.[205] Insurers wield their market power, permitting them to raise premiums without significant competitive consequence.[206]

Market concentration has exacerbated these problems.[207] Studies evaluating the effect of consolidation on pricing have found that consolidation leads to premium increases.[208] By recent measure, the top five largest insurers hold 83% of the market.[209] It seemed the so-called ※big five§ might even further consolidate down to three before antitrust challenges ultimately ended Aetna＊s attempt to acquire Humana[210] and Anthem＊s purchase of Cigna.[211] But even with five, market competition has not served as a strong check on insurers＊ abilities to raise premiums.

Second, regulation might limit premium increases. Depending on the state, an insurer might need to obtain prior approval of rates from the department of insurance.[212] But in other states, insurers might only be subject to ※file and use§ requirements, meaning they must file their rates, but the rates need not be approved,[213] or the requirement of providing an actuarial certificate attesting that their rates are in compliance with state law.[214] The ACA has brought some increased regulatory scrutiny to insurer premiums,[215] requiring that large rate increases be evaluated to ensure they are based on valid cost assumptions.[216]

But regardless of these constraints, it is a job of regulators to keep insurance companies solvent. So if costs increase, premium increases are typically considered justified and are approved.[217] Nothing about the rate approval process scrutinizes whether insurers have made efforts to contain costs.[218] Therefore, while there is considerable regulation governing rate increases, it has not significantly impacted insurers＊ abilities to raise rates.[219]

And where raising prices is prohibitive, insurers have made other business decisions to protect profits that are also viewed as more desirable than policing unnecessary care. For instance, insurers have simply withdrawn from non-profitable individual markets.[220] They have increased focus on administering employer ERISA plans where the employer bears the risk and the insurer is just paid a fee.[221] Or insurers have moved into other areas entirely, such as providing direct medical services.[222]

The final reason why insurers seem loathe to screen for unnecessary care involves the strong norms of physician and patient autonomy.

Particularly in matters concerning health, autonomy or self-rule is considered to be of great importance[223]〞both to patients and physicians.[224] One physician describes, in a commonly held view, how autonomy in health care would ideally function: ※Everyone in society should have access to needed health care . . . . Only the physician and the patient should decide how to respond to a given medical condition. And someone should reimburse providers of health care at reasonable rates.§[225]

Patient autonomy and physician autonomy mean different things and have different justifications. Patient autonomy means the right to self-determination〞to make personal decisions about what happens to one＊s own body.[226] It is specifically to be valued for deontological reasons.[227] Even if patients choose poorly, pure autonomy dictates that they should nonetheless be permitted to make the choice.[228]

Respecting patient autonomy is also said to further individual well-being.[229] Only the patient truly understands her goals and preferences; therefore, the patient is best-suited to make health care decisions.[230] And a patient is deserving of autonomy, so the argument goes, because the decisions are highly personal, affecting only the individual.

The theoretical basis for physician autonomy differs. Physician autonomy is a type of professional autonomy. A professional〞someone who has ※special power and prestige§[231]〞develops a special competence.[232] In medicine specifically, physician autonomy means the freedom to make decisions on the basis of professional judgment and specialized knowledge.[233] Physician autonomy is colloquially synonymous with clinical freedom[234] and is highly valued by physicians.[235]

Both patients and physicians have been vocal about how insurers should defer to their autonomy.[236] Indeed the need to respect patient and physician autonomy was made particularly clear following the backlash against HMOs.

But even now, insurers face pressure from patients, physicians, and the organizations that represent their interests when they refuse to reimburse for care insurers deem unnecessary.[237] Take, for example, the story of Blue Cross and Blue Shield of North Carolina＊s 2010 attempt to limit reimbursement for spinal fusion surgery under specific circumstances. Its proposal was backed by significant research indicating that spinal fusion surgery was ineffective in the circumstances under which it intended to refuse reimbursement,[238] but the American Association of Neurological Surgeons and the North Carolina Spine Society complained loudly. While they argued about the well-being of their patients, perhaps most importantly, they called the insurer＊s decision an ※intrusion into the physician-patient relationship . . . .§[239] Unsurprisingly, the insurer backed down.[240]

This is just one example to highlight the power, politically and culturally, that professional and personal autonomy arguments have.[241] These strong norms deter insurers from actively screening for unnecessary care. The autonomy value has a special moral importance in health care.[242]

Therefore, although it might seem that a rational insurer should serve as a check on patient and physician decision-making to screen out unnecessary care, important impediments exist to insurers fulfilling that role.[243] The normative question of whether or not insurers＊ failure to play the part is the right result remains unanswered. The next Part argues that although insurers cannot topple autonomy rights, there is a role for them to play.

III. Between Autonomy and Death Panels: The Need for a Nudge

Policymakers and industry experts agree that reducing unnecessary care is highly desirable. This care does not help patients〞and may even harm them〞and adds huge, unnecessary cost to the delivery of health care. Although the goal is uncontroversial, there is little consensus on what role, if any, insurers (and regulators) might play in effectuating change. The next Sections argue that while the importance of protecting patient and physician autonomy forecloses insurer-driven decision-making, patient and physician autonomy should not be unfettered. There is a crucial role for insurers (and ultimately providers) to play in cabining unnecessary care. These are precisely the circumstances〞where autonomy must be respected, but decision-making is problematic〞that call for a nudge.

A. The Autonomy Value is Important but Dangerous if Unalloyed

There are many reasons to value patient autonomy, including both deontological and welfarist reasons. Patients should have the right to make very personal decisions concerning their own bodies.

And there are equally good reasons to respect providers＊ professional autonomy, not the least of which is that professional autonomy is a key component of physician career satisfaction, which in turn is significantly associated with quality of patient care.[244] Professional autonomy motivates physicians and ensures that their expertise can be brought to bear on offering the best patient care.[245]

But arguments that this autonomy should be unfettered〞an ideal of complete clinical autonomy for physicians and decision-making autonomy for patients, without concern for resources〞are highly flawed.[246] The importance of autonomy cannot mean that there is no role for insurers or regulators in care decisions.[247]

First, when left entirely to their own devices, patients and physicians do not always make decisions that ultimately result in the best care.[248] Both patients and physicians suffer from a number of systematic decision-making biases that often prevent them from choosing treatment options that most improve well-being.[249] For instance, patients suffer from affective forecasting errors that lead to systematic mispredictions about how they will adapt to certain medical conditions, and they can be prone to commission bias, where doing something seems to be a better choice than doing nothing.[250] These biases and others likely contribute to the overtreatment and mistreatment problems discussed in Part I.[251]

Physician decision-making, too, is subject to biases and likely exacerbates these patient problems. Perhaps most prominently, physicians tend to be overly optimistic, believing that negative events are less likely to occur at their hands than those of others.[252] Physician financial incentives further skew care choices away from the ones that would be best for patients.[253]

Second, the autonomy value is predicated on the assumption that individual decisions do not negatively impact other members of society. That assumption is wrong.[254] Rather, the decision to consume unnecessary care ultimately increases premiums.[255] The evidence is convincing: spending is at least 30% higher than it should be〞raising rates for all insureds.[256] Where an individual＊s exercise of autonomy negatively impacts the rights of others in the community to exercise their own autonomy rights, the case for unalloyed autonomy is significantly weakened.[257]

The importance of autonomy does not dictate the system currently in place, where physician and patient demands for medical treatment are answered largely without question.[258] At the same time, physician and patient autonomy cannot be ignored and replaced with insurer decision-making motivated by reducing cost.[259] This was the failed experiment of the 1980s and 1990s.

Given these constraints, nudges bear consideration. A nudge holds the potential to steer decisionmakers, while still protecting autonomy.

When it is important to protect decisional autonomy, but the exercise of autonomy alone results in suboptimal decision-making, the situation may be ripe for a nudge. ※Libertarian paternalism§[260] refers to a regulatory system that ※steer[s] people＊s choices in directions that will improve the choosers＊ own welfare§ without choosing for them.[261] ※Nudges§ are the methods implemented to steer people toward better decision-making.[262]

In Richard Thaler and Cass Sunstein＊s original conception, a nudge was defined as that which ※alters people＊s behavior in a predictable way without . . . significantly changing their economic incentives.§[263] Nudges encourage people to choose in one way, but leave open the possibility of making a different choice〞like a GPS that steers you in one direction, but lets you go a different way.[264]

Lawmakers and policymakers[265] are increasingly focusing on nudges because they can be effective without being coercive.[266] For instance, Great Britain changed the default on corporate pension plans to automatically enroll employees, while allowing opt-out.[267] The change resulted in significantly increased savings for retirement. [268] In many studies, nudges compare favorably to more traditional interventions like financial incentives, sometimes having a larger desired impact than more expensive and coercive tools.[269]

The term ※nudge§ describes a diverse set of potential interventions. The next section discusses the different forms that nudges can take.

Perhaps the most well-known nudge is changing a default rule.[270] For instance, changing a regime from one that requires opting in to one of automatic enrollment with the right to opt out encourages enrollment, while protecting autonomy.[271] Policy defaults have been employed to encourage individuals with large mortgages to escrow funds for their taxes and insurance[272] and to nudge voters to vote by automatically registering citizens, but providing information on how to decline registration.[273]

Nudges, however, are not just changes to default rules. Other examples include moving healthy snacks to higher shelves than unhealthy ones,[274] redesigning a physician＊s electronic prescribing pad to make it easier for the physician to prescribe generic medication and more onerous to prescribe a brand name drug,[275] and informing customers about their neighbor＊s lower electricity usage to encourage energy conservation.[276] A vast literature has developed in the last two decades that explores nudges intended ※to make people healthier, wealthier, and happier.§[277]

A warning is another particular type of nudge.[278] Consider, for example, the surgeon general＊s warnings that are mandated to appear on cigarette packages, informing individuals that smoking causes lung cancer.[279] Credit card bills warn about the dangers in only making the minimum payment.[280] In general, warnings are intended to trigger an individual＊s attention. As Sunstein has noted, ※attention is a scarce resource, and warnings are attentive to that fact.§[281]

Warnings serve a number of purposes. In the most basic sense, they are informational.[282] They might inform a consumer of information necessary to make a decision that the consumer did not previously possess.

Warnings also serve to remind. Even if a decisionmaker might have previously known the information, she might not draw on the information in the moment of making the decision. Or because decisionmakers use judgment heuristics〞essentially mental shortcuts〞in making decisions, warnings can counteract incorrect or inaccurate recall.[283] For instance, warnings can work to counteract human tendency to be overly optimistic, in addition to other biases.[284]

Ultimately, the purpose of a warning is to influence behavior.[285] However, warnings have the advantage of being fully transparent,[286] and they do not impact personal autonomy. People are entirely free to ignore the warning if they choose.[287] Perhaps for this reason, warnings have been popular with both policymakers and with the individuals that they impact.[288]

Although warnings are not foolproof, there is ample empirical support for their effectiveness.[289] For example, consider a study of a warning nudge used to encourage students to make healthier selections in their school lunches. There, a computerized ordering system based on the U.S. Department of Agriculture MyPlate recommendations was used to encourage students to make healthier lunch choices. The study found that the treatment group subject to the nudge made significantly healthier food selections than the students who were not nudged.[290]

Not all warnings are successful, but a large and growing body of literature has explored the conditions under which warnings are most likely to result in the desired behavior. The next Section explores this literature and the challenges to the use of nudges more generally.

There are some general criticisms of the use of nudges.[291] While less paternalistic than flat out regulation, nudges can still be coercive. If one believes that government should never intervene in an individual＊s sovereign life,[292] then nudging is an unwanted intrusion. One critic of nudging has described nudges as ※state incursions into the sphere of liberty.§[293]

For instance, the government might want to try to reduce obesity by drawing attention to the calories in fast food.[294] If one believes that the choice to be obese is one that a free individual in a free society should be able to make〞and that an individual should not be forced to know how many calories they are consuming〞then mandatory calorie labeling may be an unwanted governmental intrusion.

Nudging is also criticized when the goals behind its use are controversial. Put another way, the choice architect that puts in place the nudge is making certain decisions about what behavior is desirable. That choice might reflect a value judgment about which people disagree. Or it might be based on data that is imperfect or subject to interpretation.

Finally, not all nudges actually work, in the sense of accomplishing the behavior change that they intend. And although nudges are intended to be low cost, they are often not costless. There is the risk that nudging＊s costs exceed its benefits in some circumstances.

Although these objections bear consideration, they are not insurmountable. Nudges do involve a degree of manipulation, but then so do essentially all decisions. It is difficult to avoid all aspects of paternalism in presenting choices. As to concerns about deciding which way to nudge, these are valid, but less problematic when the nudge is transparent.[295] If the government is the choice architect and people do not like the direction the government is nudging, they can make their opinions known by voting. If done right, nudges are transparent and preserve autonomy, muting concerns about infringement on liberty.[296] However, nudges are problematic if they are costly or do not work, which is why the gathering of good empirical data is crucial.

The use of warnings to nudge raises some additional concerns. Because warning nudges are perhaps the least coercive of all nudges, the biggest question is whether or not they work to influence choice.[297] On this question, the literature is robust. Scholars have identified the following crucial factors in determining the success of warning nudges: (i) noticeability, (ii) temporal and spatial proximity, (iii) the requirement of physical interaction with the warning, (iv) the use of concrete directions, (v) and the cost associated with warning compliance.[298]

First, warnings are likely to be most effective where they are conspicuous and brief.[299] Few people actively look for warnings. If the warning is not read by the decisionmaker, it cannot have the desired effect.[300]

Second, temporal and spatial proximity are important to warning compliance. For instance, in one study, a warning that a file cabinet would tip if the top cabinet were filled before the bottom cabinet was placed on the packing box and in various places inside the top cabinet.[301] When the warning was just on the packing box, it was viewed, read, and complied with at significantly lower rates than when it was placed inside the cabinet. Further, when the warning was placed on a cardboard bridge that had to be physically removed to fill the top drawer, compliance levels were even higher than when the warning was simply placed inside the cabinet.[302] Results from other studies are consistent: compliance is highest when the warning is delivered in both physical and temporal proximity to the task and when the decisionmaker must physically interact with it in some way.[303] Given these parameters, technology provides particularly good opportunities to deliver effective warnings at a low cost.[304]

Third, warnings are more likely to be effective if accompanied by concrete directions for the individual reading it, such as you can do X and Y to lower your risk.[305] In one study, people who used water-repellent sealer were given different labels to determine whether compliance is dependent upon the explicitness of warnings and instructions.[306] The results indicated that ※procedurally explicit precautions included in the directions substantially increased reading rates from 4% to 78% and compliance rates from 10% to 65%.§[307]

Lastly, to the extent that decisionmakers view the decision of whether or not to comply with a warning through the lens of a cost-benefit analysis, then the cost of complying with a warning is particularly important and can influence warning effectiveness.[308] Compliance cost may take many forms. There may be financial costs associated with warning compliance, or there may be costs in terms of time, convenience, or emotional toll.[309] For instance, a smoker may choose to ignore the surgeon general＊s warning because the costs of recovering from addiction are higher than the cost of good health〞at least in that individual＊s estimation. Biases can skew cost-benefit analyses.[310] In particular, the optimism bias, which causes people to discount the risk of experiencing a negative event, can be impactful. But in general, studies confirm that the lower the costs of complying with the warning, the more likely that the warning will be effective.[311]

Warnings are not perfect. Users can choose to ignore them. Plenty of people still smoke despite surgeon general＊s warnings. And many still only make the minimum payment on their credit card bills. Warnings cannot always counteract other, potentially stronger forces motivating decisions. But nonetheless, there is reason to be optimistic that a properly designed warning that accounts for these challenges could effectively reduce unnecessary care. The next Part presents the proposal.

The fundamental challenge with reducing unnecessary care is that there is a spectrum from patient and physician autonomy on one end to insurer decision-making on the other, where both poles are problematic, and no path to a middle ground has been identified. Complete autonomy for patients and doctors results in health care decisions being made by those with no real incentive to stop harmful overuse. But too much control by insurers means that both non-beneficial and beneficial care may be rationed.

In Crossing the Quality Chasm, the Institute of Medicine delivered the directive that, ※IT must play a central role in the redesign of the health care system if a substantial improvement in health care quality is to be achieved during the coming decade.§[312] In keeping with that sentiment, the way to reduce unnecessary care, but protect patient and physician autonomy, is by employing a computerized nudge. Providers should receive an automated warning before their orders for commonly overused interventions are placed, dissuading unnecessary care without the use of inflexible, autonomy-reducing methods. In a perfect world, insurers would require this nudge in their contracts with providers. However, because a toxic combination of mismatched legal incentives, market failures, and industry norms prevents that from happening, the government will have to mandate it.

The health care industry is notoriously behind in its adoption of technology.[313] Yet recent years have been marked by a digital transformation of medicine.[314] Use of health IT systems and electronic health records is now widespread and growing.[315] ※By the end of 2017, approximately 90% of office-based physicians nationwide will be using electronic health records.§[316] The vast majority of hospitals and most outpatient practices now use some form of Computerized Provider Order Entry (※CPOE§) system, which allows providers to order tests, procedures, consultations, and prescribe medications electronically.[317] Most communications between providers and insurers now also occur electronically, with use of electronic prior authorization procedures notably on the rise. These systems are far from perfect, often scoring low on usability and interoperability.[318] Lawmakers and policymakers are continuing to explore ways to improve the state of health IT.[319] But this technology proliferation can be deployed to effectively nudge.

Systems could be programmed so that a computerized warning pops up when a provider places an order (or submits a pre-authorization) for a test or procedure that is likely to be unnecessary.[320] The warning would prompt the provider with a question that essentially asks if she is sure about the order given evidence that the test or procedure is often overutilized or misconsumed. The warning should link to additional sources of information that supply the warning＊s scientific basis to the provider. And where appropriate, the warning should also suggest an alternate intervention.

For instance, when a provider enters an order for an X-ray for uncomplicated lower back pain, a CT scan for sinusitis, or an MRI for an uncomplicated headache〞all of which have been identified as commonly overused tests[321]〞the warning would pop up. Similarly, to address misconsumption, a warning could pop up when a provider tries to schedule (or seeks pre-authorization for) a spinal fusion surgery for a worn out disc or laparoscopic uterine nerve ablation for chronic pelvic pain.[322] The provider could proceed as planned if the provider believes that it is in the best interests of the patient to do so, but this nudge requires the provider to click ※Yes, I＊m sure§ for the order to process.[323] Although the warning would not deter all unnecessary care, there is reason to believe it could be effective.

There is precedent for such a system already in place. Clinical decision support systems (※CDSSs§) are designed to help providers make care recommendations to their patients. Early CDSS systems were used in prescribing medications, attempting to prevent medication errors by screening for drug doses, allergies, and drug interactions. CDSSs now go beyond a focus on medications and often employ alerts and warnings relevant to broader issues of patient care.[324] Some insurer systems are also screening for red flags of unnecessary care, typically as a part of their electronic pre-authorization practices[325]〞although the purpose of these reviews is ultimately to decide whether or not to grant pre-authorization. Providers cannot override denials.[326]

CDSSs, however, are not yet in widespread use[327] and, as currently implemented, are both over每inclusive and under-inclusive. CDSSs, particularly as employed for e-prescribing, have been criticized for the abundance of alerts that pop up when providers try to enter orders, many of which are not useful.[328] And even where CDSSs are deployed, they are still mostly used for prescribing and not for other types of orders and interventions. There is also concern about the quality of the data on which these systems are programmed.[329] The next Section takes up the data issue specifically.

1. The Data on Which to Base Warnings

Perhaps one of the biggest questions about any computerized warning system concerns the underlying data. Who decides which orders draw warnings and how?[330] This Section makes a suggestion, but also provides alternatives worth considering.

The first, and probably best option, is to tie the warnings to the already established ※Choosing Wisely§ campaign. In 2012, the American Board of Internal Medicine Foundation, Consumer Reports, and nine medical specialty societies launched the campaign.[331] Its purpose was to promote quality, evidence-based care that was ※truly necessary.§[332] Specifically, it tasked medical societies with preparing lists of five tests or procedures in their clinical domains that are performed too often.[333] The initial set of lists was well-received, and the campaign has grown from there, with more than seventy societies now involved.[334] The Robert Wood Johnson Foundation has provided considerable funding to aid its efforts, awarding grants in 2013 and 2015.[335]

To give an example of how the campaign works, the American College of Radiology (※ACR§)〞a nonprofit, professional medical society whose members are radiologists, nuclear medicine physicians, and medical physicists[336]〞generated its list of five recommendations to reduce unnecessary care in radiology, one of which states simply: ※Don＊t do imaging for uncomplicated headache.§[337] To support the recommendation, ACR provides citation to a list of academic sources.[338] This guidance could easily be converted to a computerized warning, such that a provider entering an order for imaging for an uncomplicated headache would get a pop-up discouraging (but not preventing) the order.

The Choosing Wisely lists have been well-received, but on their own have not had a particularly robust effect on unnecessary care. One study found that following the release of the guidelines, use of imaging for headaches and cardiology both declined, but other services on the ※unnecessary§ lists actually saw increases in use.[339] Merely establishing guidelines does not seem to be enough to effect practice patterns.

Using professional societies more generally〞and the Choosing Wisely lists specifically〞as the basis for computerized warnings, however, offers a number of advantages. The lists are driven by the providers practicing in their respective specialties. They are simple to translate to warnings and are not so numerous as to overwhelm. They do not contain ※close calls,§ but rather focus on the big-ticket sources of overuse. They also establish a national standard, which might even be helpful to physicians in malpractice litigation[340] and could promote uniformity in approach.[341]

In terms of challenges, however, the Choosing Wisely lists may reflect and perpetuate physician biases in care given that it is the providers themselves generating the lists. If the project gets into more controversial territory, it might be hard to reach consensus. There would need to be a structure for updating the lists over time, and the project＊s funding is driven by private grants, which may not sustain it over the long term.

There are other options for sourcing the data. Government-driven processes have the advantage of solving the funding problem and ensuring a singular national approach. Any government process would need to win the buy每in of the provider community, though, and would need to overcome the traditional hurdles of bureaucracy and difficulty in ensuring quick responses to changes in evidence. A government-driven process would likely meet great resistance from the current Congress and administration.[342]

Another option is a government-private partnership. The Advancing Clinical Decision Support project, led by the RAND Corporation, Massachusetts-based Partners HealthCare, and Harvard Medical School and funded by the U.S. Office of the National Coordinator for Health Information Technology is an example already underway. The overall goal of the project is to address barriers to adopting CDSSs. Partnerships such as this could be tasked with vetting data for the warning nudges.[343]

Finally, there are a number of dispersed private endeavors engaged in evaluating scientific evidence, typically with a focus on new medical technologies.[344] These endeavors are driven by health plans, but usually include representation from a wider community of industry actors, such as academics, physicians, and representatives from medical associations.[345]

Ultimately, the goal would be to use sound scientific data that is consensus-driven, with significant buy-in from the providers that the data seeks to influence. Where clear data is lacking, a warning would not be used. There are plenty of areas, however, where unnecessary care data is robust; efforts should start there. Over time, the warnings should be personalized as effectively as possible to individual patient circumstances. Although there is much work to be done, the trend is moving strongly in that direction.[346]

2. Why Warning Nudges Would Be Effective

Warning messages based on Choosing Wisely lists or other comparable data could effectively reduce unnecessary care. The warnings utilize most, if not all, of the best practices scholars have identified for securing warning compliance and do not significantly impact physician autonomy, which is perhaps the biggest concern in any effort to reduce unnecessary care.

First, the warnings would disseminate valuable, targeted information. The timing and mode of delivery ensures that providers will draw on relevant information at the time of deciding on care. The warnings would be designed to force physical interaction because the provider will have to click to ※answer§ it.[347] Where possible, the warnings would also provide a concrete suggestion of what the provider should do instead of the original order.[348] Importantly, the warnings would be limited and targeted because the Choosing Wisely lists are limited and targeted, addressing a major source of provider ire with current CDSS systems that over-warn and prompt warning fatigue.

Second, although designed to prompt compliance, the warnings would not impact professional autonomy. Ultimately, providers still decide whether or not to place the order. Providers often fight the use of evidence-based medicine to the extent it impacts their abilities to take account of differences between individual patients and does not respect their professional intuition.[349] The warning system addresses those arguments by nudging providers away from care that is likely unnecessary, while avoiding a more coercive approach.

Early studies provide further basis for optimism that unnecessary care warnings can be effective. Los Angeles-based Cedars-Sinai Health System ※built 180 Choosing Wisely recommendations into its electronic health records . . . .§[350] For instance, because sedative drugs are not supposed to be used by the elderly, it built in a warning for new orders of benzodiazepine for patients over age 65. The health system saw a 40% decrease in use after implementing the warning message.[351] New York-based Crystal Run Healthcare experimented with something similar, programming into its system pop-up alerts based on four of the Choosing Wisely recommendations from the American Academy of Family Physicians. It led to decreases in annual EKGs, MRIs for low back pain, and bone density screening. [352]

More generally, there is ample evidence that warnings built into computerized provider order entry systems can improve the quality and efficiency of care. In Scotland, the addition of computerized, educational reminder messages was associated with a reduction of more than 20% in test ordering.[353] Another study found increased adherence to radiology test guidelines and a decrease in radiology utilization.[354] There are also many studies of medication alerts that have been shown to be effective in reducing unnecessary and even harmful prescriptions.[355] For instance, in one study, physicians who received computerized alerts about a drug＊s potential adverse effects were significantly less likely to prescribe the medication.[356] Many other systematic reviews have found that computerized decision support tools in general can deter low-value care.[357]

Given the promising nature of this approach, it might be surprising that it has not gained significant traction. The next Section explains why the government will need to mandate the nudge.

B. The Need to Mandate This Nudge

Insurers would be best situated to require unnecessary care warnings because providers lack sufficient incentive to adopt such technology. While some health systems have experimented with implementing these warnings unprompted by insurers, it is unlikely that these systems will proliferate if left to providers. After all, despite the beginning of payment reform, providers are still largely paid on a fee-for-service basis, giving them the financial incentive to order more care, not less. Providers must pay to implement the technology, and while providers may have incentive to improve quality〞which should mean reducing harmful unnecessary care〞it is hard to see much effort originating from health systems or provider groups absent other forces at play.[358]

Insurers, however, could require by contract an unnecessary care warning system. Private insurers and providers routinely enter into contracts that govern provider ※in network§ status and dictate reimbursement rates, among other provisions. There is nothing that prevents insurers〞especially because most possess a dominant negotiating position over providers[359]〞from simply requiring by contract that provider每ordering systems include warnings of unnecessary care.

Flaws in decision-making caused by market failures and counterproductive legal incentives have, however, gotten in the way.[360] The most prominent flaw being that it is easier to raise premiums in an imperfectly competitive, ineffectively regulated market, than to undertake systematic efforts to reduce unnecessary care that involve incurring transaction costs.[361] For this reason, the nudge will not happen unless it is mandated.

It is not uncommon for nudges to require regulation.[362] For instance, in an effort to reduce obesity, lawmakers required the labeling of nutrition information on restaurant menus.[363] Other so-called ※informational nudges§ have also been regulated,[364] such as laws pertaining to credit card disclosures, insurance choice, and fuel economy, to name a few.[365]

And insurance contracts are already subject to a host of other mandates.[366] In the name of consumer protection and public health, laws require coverage of essential health benefits, among other services or benefits for which insurers must provide coverage, including coverage for particular types of providers and coverage for certain groups like adopted children and domestic partners.[367]

Mandates generally are not without controversy. They infringe on the parties＊ freedom of contract.[368] And there is concern that insurance mandates increase premiums,[369] but if the system works as designed, a mandate requiring unnecessary warnings should actually decrease premiums by reducing costly unnecessary care. Any additional cost that the specific mandate contemplated in this Article provokes should be covered by the anticipated savings.[370]

Lawmakers might also avoid the insurance contract and instead go directly to the source by requiring that providers implement these warning systems. This direct regulation might be conducted through amendments to HIPAA or perhaps through the meaningful use requirements. But insurers are still the actors most motivated to reduce unnecessary care, despite the challenges, and it makes the most sense to give them a crucial role to play, particularly in monitoring for compliance. Although insurers might not have the incentives to require a warning nudge system in the first place, once it is in place, they will have an incentive to see it work effectively.

C. Addressing Challenges and Suggestions for Further Study

Although the warning nudge has the potential to substantially reduce unnecessary care and drive down insurance premiums, it is not without its challenges.

The most commonly levied argument against nudging is that it is paternalistic and manipulative, even if it might be less overtly coercive than other forms of regulation.[371] The argument against automated warnings, then, is perhaps obvious〞they are an attempt to change behavior and do not respect the autonomy of doctor and patient decision-making.

It is true that the whole purpose of the warning system would be to influence decision-making and change behaviors. But while there is a degree of paternalism, any coercion is minimal. After all, doctors and patients are ultimately free to make their own decisions absent repercussion. Common alternatives, such as tying compensation to the likely effectiveness of treatment, are far more coercive.

In general, health care interactions already contain all sorts of attempts to influence. Doctor treatment recommendations in particular have been shown to highly influence the choices that patients make about their care.[372] Framing effects in health care are, to a large degree, unavoidable. And if that is the case, the arguments for at least framing in the ※right§ direction are strong.

But what is the ※right§ direction? Perhaps the biggest stumbling block to the nudge solution is still the data. As scholars have noted, ※[d]espite widespread concerns that many patients receive biomedical interventions that have little or no evidence about their safety and therapeutic effectiveness, what ＆effective＊ means and how to measure it are contested questions.§[373] Indeed much of medical practice is the result of tradition and collective experience, not the rigorous and systematic study of medical interventions.[374] And the individual skill of a provider or individual patient characteristics can often be highly relevant to outcomes.[375]

Yet while it is undoubtedly true that definitive clinical information about effectiveness is often lacking and individual circumstances may often be important, this need not be paralyzing. More studies need to be done, but additional resources are being directed at learning more about the effectiveness of a wide-range of treatments. And scholars and policymakers have good ideas about how to incentivize more useful clinical research.[376]

It is clear that evidence of overuse and misuse currently exists. It would be short-sighted to ignore that data〞that campaigns like Choosing Wisely have studied〞and take no action while waiting for more comprehensive data to emerge. Even if data is imperfect, the warning nudge accounts for personalized decision-making by not requiring compliance. Providers may still disagree. This is a key advantage over other proposed solutions. For instance, one idea that has started to gain traction is for insurers to tier reimbursement or cost-sharing rates to evidence of effectiveness. This is promising, but the concern about infringement on physician judgment is an important stumbling block to this more aggressive nudge.[377]

The fact that evidence of effectiveness (or ineffectiveness) is lacking for many interventions is not necessarily a bad thing for a nascent alert system. From a cognitive perspective, it may be beneficial to focus on a smaller number of (high impact) warnings. Too many warnings may be counterproductive.

Another challenge concerns potential provider resistance to a warning system. Providers as a group are fiercely protective of their professional autonomy. This has translated into some general resistance to the concept of evidence-based medicine, which is perceived as a slight to professional judgment and intuition.[378]

But there is evidence that physicians can be influenced by computerized warning systems,[379] and a number of reasons exist to believe that provider buy-in can be obtained in this context. First, if the warnings are driven by the work of professional associations, of which providers are members, they are more likely to credit the warnings and pay attention. Second, warnings are much less coercive and have a far less serious impact on professional autonomy than other methods of reducing unnecessary care, like utilization reviews or even value-based insurance design.[380] Third, physicians desiring to limit their practice of defensive medicine might feel empowered by the national standards.[381] A good warning system should actually reduce liability that stems from unnecessary care that causes harm.

A related challenge to obtaining provider buy in concerns provider resistance to IT. One survey found that doctors in the United States were less likely to believe that health IT can improve care than in other countries.[382] Physicians are often suspicious that technology will lead to the industrialization of medicine and complain that IT adoption has increased their administrative burdens, giving them less time to focus on important patient care.[383] Studies indicate that physicians ※are reluctant to support an IT that interferes with their traditional work routines.§[384] Notably, for this alert to be effective, it must be communicated at a time when it can influence the doctor＊s care recommendations, which may involve changes to work routines. Many doctors currently don＊t access an order entry system until after a course of care has been agreed to with the patient.[385]

Alert fatigue〞the idea that with too many (unhelpful) alerts, providers will stop paying attention〞is also a concern.[386] Some particularly troubling reviews have found very high alert override rates, particularly where alerts are pervasive and important alerts are buried in a sea of unimportant ones.[387]

It is therefore particularly important to design a warning system that does not overwhelm. Better to use discrete lists, like the ones from Choosing Wisely＊s campaign, that might under-deter unnecessary care than to risk undermining the entire enterprise by adding too many unwieldy alerts.

It is also important to test for unintended consequences. For instance, it is possible that physicians might react negatively to the alerts and increase unnecessary care rather than reduce it. This reaction seems unlikely, particularly when coupled with payment reform and the fact that these alerts would be driven by peers. But with any new nudges, experimentation is prudent to test for any potentially negative consequences.[388]

On the other hand, if the nudge is not strong enough, insurers can be utilized to encourage compliance〞another advantage of involving insurers. For instance, an insurer could either tier reimbursement rates such that the care it wishes to deter is reimbursed at a lower rate or could tier co-pays to try to directly impact patient decision-making.[389] There are downsides to this, particularly that this more coercive use of financial incentives requires an even higher confidence in the data and more negatively impacts patient and physician autonomy.[390] But it is an option that could be deployed if provider resistance to the warnings proves too strong.

Another crucial consideration is the cost of provider compliance.[391] The administrative burden discussed in the prior Section is one such cost. Presumably that cost could be minimized by systemic efficiencies and targeted use of warnings. But there is another relevant provider cost〞the revenue providers will lose from delivering less care (or care with lower rates of reimbursement) in a fee-for-service system.

This is an important cost. One could imagine a rational physician who is warned that a spinal fusion surgery is likely unnecessary, but discounts that warning because he stands to earn a significant amount from performing the surgery. In a world where the warnings are easily ignored and the reimbursement rates from unnecessary care are substantial, warnings may not be particularly effective.

Here, there are at least two answers. First, the payment reform movement might be helpful. If providers are paid based on patient outcomes and cost savings, rather than on a fee-for-service basis, this problem becomes much less significant.[392] Although there has been a big trend away from fee-for-service, a few politicians have expressed skepticism of payment reform. The future is unclear.

Second, cultural forces (and pressures) are not to be underestimated and might play a more important role in physician decision-making than rote financial concerns. If these warnings are generated by professional associations of which the provider is a member and effectively change the standard of care, it might override reimbursement concerns. This is particularly true if the warnings cause the standard of care to more quickly align with the evidence.

Finally, although the nudge is directed at physician practices, it is actually patients who ultimately make care decisions. Informed consent doctrine requires that a physician explain treatment options and attendant risks and benefits, so that the patient can properly decide on care.[393] Therefore, reducing unnecessary care will not be accomplished by directing efforts at providers alone.

That being said, if the care is truly unnecessary, the physician is not legally required to give the patient the option to choose it.[394] And as a practical matter, physicians are highly influential in care decisions and often hold the ability to align patient perceptions about appropriate care with their own.[395] Most patients are not as medically literate as their doctors. Some patients would actually prefer to have the physician decide their care, or at least strongly steer them in the right direction.[396] To the extent that physicians today acquiesce to patient demands for unnecessary care more often than they should, the warning system might give the physician more confidence to persuade the patient otherwise.

At bottom, all of these potential challenges could be better assessed with more empirical testing of the concept. Responses to nudges are notoriously unpredictable. But given the magnitude of the unnecessary care problem and the fundamental flaws in the other approaches, now is the time to try.[397]

High premiums have become the central focus of recent debates over the U.S. healthcare system＊s future. Solutions like reducing benefits or kicking sick people out of risk pools would have dire consequences. There is a better target〞the unnecessary care that accounts for 30% of health care costs.

The problem is well-documented. Patients and providers lack adequate incentive to address it. Insurers would seem best situated to act as a check because a rational insurer should want to reduce claims costs. But insurer experiments with refusing to reimburse for unnecessary care were political failures. And because of problematic legal incentives, market failures, and cultural norms, insurers have taken a back seat to patients and physicians in recent years.

The problem calls for a nudge〞a way to respect patient and physician autonomy, but to discourage unnecessary care. A computerized warning, based on the professional consensus of medical associations, could be particularly impactful and should be implemented and tested. In a perfect world, insurers would just require providers they contract with to implement the warnings. But in this case, the government will likely have to intervene.