{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, fbeta_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"input/train_v2.csv\")\n",
    "test_data = pd.read_csv(\"input/test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Forex - Pound drops to one-month lows against ...</td>\n",
       "      <td>http://www.nasdaq.com/article/forex-pound-drop...</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>www.nasdaq.com</td>\n",
       "      <td>1.390000e+12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0           1  Forex - Pound drops to one-month lows against ...   \n",
       "\n",
       "                                                 url publisher  \\\n",
       "0  http://www.nasdaq.com/article/forex-pound-drop...    NASDAQ   \n",
       "\n",
       "         hostname     timestamp  category  \n",
       "0  www.nasdaq.com  1.390000e+12         4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_warn_or_error_list(filename):\n",
    "    with open(filename, errors='ignore') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    _list = []\n",
    "    for line in lines:\n",
    "        _list.append(line.split(',')[0])\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success_train = 'raw_train_new/success.txt'\n",
    "success_test = 'raw_test_new/success.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success_train_list = read_warn_or_error_list(success_train)\n",
    "success_test_list = read_warn_or_error_list(success_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_fulltext_list = []\n",
    "# for i in train_data['article_id']:\n",
    "#     if str(i) in success_train_list:\n",
    "#         filename='raw_train_new/'+str(i)+'.txt'\n",
    "#         with open(filename, 'r',errors='ignore') as data_file:\n",
    "#             train_fulltext_list.append(train_data[train_data.article_id == i].title.iloc[0] * TITLE_REPLICATES+ ' ' + data_file.read().replace('\\n', ''))\n",
    "#     else:\n",
    "#         train_fulltext_list.append(train_data[train_data.article_id == i].title.iloc[0] * TITLE_REPLICATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_fulltext_list = []\n",
    "# for i in test_data['article_id']:\n",
    "#     if str(i) in success_test_list:\n",
    "#         filename='raw_test_new/'+str(i)+'.txt'\n",
    "#         with open(filename, 'r',errors='ignore') as data_file:\n",
    "#             test_fulltext_list.append(test_data[test_data.article_id == i].title.iloc[0] * TITLE_REPLICATES+ ' ' + data_file.read().replace('\\n', ''))\n",
    "#     else:\n",
    "#         test_fulltext_list.append(test_data[test_data.article_id == i].title.iloc[0] * TITLE_REPLICATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#v2: remove numbers and punctuations\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "train_data['text']=\"\"\n",
    "#train_data['title_clean']=\"\"\n",
    "for i in range(len(train_data['title'])):\n",
    "    filename='raw_train_new/'+str(i+1)+'.txt'\n",
    "    title=train_data['title'][i]\n",
    "    title = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", title)\n",
    "    title = ''.join(filter(whitelist.__contains__, title)).lower()\n",
    "    title = ' '.join(title.split())\n",
    "    # train_data['title_clean'][i]=title\n",
    "    #add space to title\n",
    "    train_data['title'][i]=title\n",
    "    title=title+\" \"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r',errors='ignore') as data_file:\n",
    "            data=data_file.read().replace('\\n', '')\n",
    "            data = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", data)\n",
    "            data = ''.join(filter(whitelist.__contains__, data)).lower()\n",
    "            data=' '.join(data.split())\n",
    "        train_data['text'][i]=data  # title*TITLE_REPLICATES+data\n",
    "    else:\n",
    "        train_data['text'][i]=\"\"  # title*TITLE_REPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#v2 remove numbers and punctuations\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "test_data['text']=\"\"\n",
    "#test_data['title_clean']=\"\"\n",
    "for i in range(len(test_data['title'])):\n",
    "    filename='raw_test_new/'+str(i+1)+'.txt'\n",
    "    title=test_data['title'][i]\n",
    "    title = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", title)\n",
    "    title = ''.join(filter(whitelist.__contains__, title)).lower()\n",
    "    title = ' '.join(title.split())\n",
    "    #test_data['title_clean'][i]=title\n",
    "    test_data['title'][i]=title\n",
    "    title=title+\" \"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r',errors='ignore') as data_file:\n",
    "            data=data_file.read().replace('\\n', '')\n",
    "            data = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", data)\n",
    "            data = ''.join(filter(whitelist.__contains__, data)).lower()\n",
    "            data=' '.join(data.split())\n",
    "        test_data['text'][i]=data  # title*TITLE_REPLICATES +data\n",
    "    else:\n",
    "        test_data['text'][i]=\"\"  # title*TITLE_REPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.assign(original_text=train_data.text)\n",
    "test_data = test_data.assign(original_text=test_data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(sent, stemmer_type='snowball'):\n",
    "    '''\n",
    "    stemmer_type can be porter, lancaster, or snowball\n",
    "    '''\n",
    "    if stemmer_type == 'porter':\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer_type == 'lancaster':\n",
    "        stemmer = LancasterStemmer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    words = gensim.utils.simple_preprocess(sent)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #trainf='train_full_clean.txt'\n",
    "# trainf='train_full.txt'\n",
    "# #X = pd.read_csv(trainf,header=None,sep=\",\",names=['data'])\n",
    "# with open(trainf) as f:\n",
    "#     X = f.read().splitlines()\n",
    "\n",
    "# df = pd.DataFrame({\"data\": train_fulltext_list})\n",
    "# df = df.assign(article_id=train_data.article_id, category=train_data.category)\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #trainf='train_full_clean.txt'\n",
    "# trainf='test_full.txt'\n",
    "# #X = pd.read_csv(trainf,header=None,sep=\",\",names=['data'])\n",
    "# with open(trainf) as f:\n",
    "#     X = f.read().splitlines()\n",
    "\n",
    "# df_test = pd.DataFrame({\"data\": test_fulltext_list})\n",
    "# df_test = df_test.assign(article_id=test_data.article_id)\n",
    "# df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_noerror = df[df.article_id.isin(success_train_list)]\n",
    "df_noerror = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test = df_test[df_test.article_id.isin(success_train_list)]\n",
    "df_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform stemming\n",
    "df_noerror.loc[:, \"original_text\"] = df_noerror[\"original_text\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_noerror.loc[:, \"title\"] = df_noerror[\"title\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.loc[:, \"original_text\"] = df_test[\"original_text\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.loc[:, \"title\"] = df_test[\"title\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TITLE_REPLICATES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_noerror.loc[:, \"text\"] = (df_noerror[\"title\"] + \" \")*TITLE_REPLICATES + df_noerror[\"original_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.loc[:, \"text\"] = (df_test[\"title\"] + \" \")*TITLE_REPLICATES + df_test[\"original_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6027, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noerror.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3826, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>forex pound drop to one month low against euro</td>\n",
       "      <td>http://www.nasdaq.com/article/forex-pound-drop...</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>www.nasdaq.com</td>\n",
       "      <td>1.390000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>forex pound drop to one month low against euro...</td>\n",
       "      <td>invest com the pound fell to one month low aga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                           title  \\\n",
       "0           1  forex pound drop to one month low against euro   \n",
       "\n",
       "                                                 url publisher  \\\n",
       "0  http://www.nasdaq.com/article/forex-pound-drop...    NASDAQ   \n",
       "\n",
       "         hostname     timestamp  category  \\\n",
       "0  www.nasdaq.com  1.390000e+12         4   \n",
       "\n",
       "                                                text  \\\n",
       "0  forex pound drop to one month low against euro...   \n",
       "\n",
       "                                       original_text  \n",
       "0  invest com the pound fell to one month low aga...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noerror.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>white hous play down speedi role for us natur ...</td>\n",
       "      <td>http://www.thestar.com.my/News/World/2014/03/0...</td>\n",
       "      <td>The Star Online</td>\n",
       "      <td>www.thestar.com.my</td>\n",
       "      <td>1.390000e+12</td>\n",
       "      <td>white hous play down speedi role for us natur ...</td>\n",
       "      <td>on board air forc one reuter the white hous on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0           1  white hous play down speedi role for us natur ...   \n",
       "\n",
       "                                                 url        publisher  \\\n",
       "0  http://www.thestar.com.my/News/World/2014/03/0...  The Star Online   \n",
       "\n",
       "             hostname     timestamp  \\\n",
       "0  www.thestar.com.my  1.390000e+12   \n",
       "\n",
       "                                                text  \\\n",
       "0  white hous play down speedi role for us natur ...   \n",
       "\n",
       "                                       original_text  \n",
       "0  on board air forc one reuter the white hous on...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a21bdf2e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHTMEJKljhoAkdez/AGhX6eqOqoXlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a220fc358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_noerror.category.value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label_train, feature_vector_valid, label_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label_train)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return {'accuracy': accuracy_score(label_valid, predictions), \n",
    "            # 'f2_seperate': fbeta_score(Y_test, Y_predict, average=None, beta=2),\n",
    "            'f2_macro': fbeta_score(label_valid, predictions, average='macro', beta=2),\n",
    "            'f2_micro': fbeta_score(label_valid, predictions, average='micro', beta=2),\n",
    "            'f2_weighted': fbeta_score(label_valid, predictions, average='weighted', beta=2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_data(train_data):\n",
    "    X = train_data[\"text\"].values\n",
    "    y = train_data[\"category\"].values\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        yield X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # use both title and content to fit tfidf\n",
    "# tfidf_vect_ngram = TfidfVectorizer(analyzer='word',\n",
    "#                                    # token_pattern=r'\\w{1,}', \n",
    "#                                    ngram_range=(1,3), \n",
    "#                                    max_df=1.0,\n",
    "#                                    min_df=1,\n",
    "#                                    max_features=3500)\n",
    "\n",
    "# tfidf_vect_ngram.fit(df_noerror[\"data\"].tolist() + df_test[\"data\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use title only to fit tfidf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',\n",
    "                                   # token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(1,3), \n",
    "                                   max_df=1.0,\n",
    "                                   min_df=1,\n",
    "                                   max_features=3500)\n",
    "\n",
    "tfidf_vect_ngram.fit(train_data[\"title\"].tolist() + test_data[\"title\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha=0.8\n",
    "0.6466\n",
    "\n",
    "0.5\n",
    "0.6432\n",
    "\n",
    "0.9\n",
    "0.6492\n",
    "\n",
    "0.95\n",
    "0.6497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6500829187396352}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6376451077943616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6517412935323383}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6434494195688225}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6616915422885572}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6600331674958541}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6376451077943616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6592039800995025}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6774461028192371}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6185737976782753}\n",
      "mean accuracy: 0.6497512437810946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    }
   ],
   "source": [
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "#     clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "    lp_model = LabelSpreading(kernel='rbf',\n",
    "                              gamma=10, \n",
    "#                               n_neighbors=25,\n",
    "                              alpha=0.95,\n",
    "                              max_iter=50,\n",
    "                              n_jobs=6)\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    xsemi_tfidf_ngram = tfidf_vect_ngram.transform(np.concatenate([X_test, df_test[\"text\"]]))\n",
    "\n",
    "    # First training using only training set\n",
    "    ysemi_unlabeld = -np.ones((X_test.shape[0]+df_test.shape[0],))\n",
    "    lp_model.fit(sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram)).toarray(),\n",
    "                 np.concatenate((Y_train, ysemi_unlabeld)))\n",
    "    predicted_labels = lp_model.transduction_[xtrain_tfidf_ngram.shape[0]:xtrain_tfidf_ngram.shape[0]+X_test.shape[0]]\n",
    "    metrics = {'accuracy': accuracy_score(Y_test, predicted_labels)}\n",
    "\n",
    "    # semi supervised stage\n",
    "#     for epoch in range(epochs):\n",
    "#         # create labels\n",
    "#         prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "#         THRESHOLD = 0.6\n",
    "#         mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "#         ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "#         print(ysemi[mask].shape)\n",
    "\n",
    "#         if epoch == epochs-1:\n",
    "#             # Use SVC for last epoch\n",
    "#             metrics = train_model(SVC(kernel='linear'),\n",
    "#                                   sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "#                                   np.concatenate((Y_train, ysemi[mask])), \n",
    "#                                   xvalid_tfidf_ngram, Y_test)\n",
    "# #             metrics = train_model(xgboost.XGBClassifier(max_depth=5, n_estimators=140),\n",
    "# #                               sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "# #                               np.concatenate((Y_train, ysemi[mask])), \n",
    "# #                               xvalid_tfidf_ngram, Y_test)\n",
    "#         else:\n",
    "#             metrics = train_model(clf,\n",
    "#                                   sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "#                                   np.concatenate((Y_train, ysemi[mask])), \n",
    "#                                   xvalid_tfidf_ngram, Y_test)\n",
    "    metrics_list.append(metrics)\n",
    "    print(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "# print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "# print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "# print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1369,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6666666666666666, 'f2_macro': 0.5663517272397725, 'f2_micro': 0.6666666666666666, 'f2_weighted': 0.658071300219084}\n",
      "(1327,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6625207296849088, 'f2_macro': 0.5366012560074029, 'f2_micro': 0.6625207296849088, 'f2_weighted': 0.6508284168551369}\n",
      "(1320,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6724709784411277, 'f2_macro': 0.5608287694439502, 'f2_micro': 0.6724709784411277, 'f2_weighted': 0.6611779036131425}\n",
      "(1368,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6674958540630183, 'f2_macro': 0.5815700878483403, 'f2_micro': 0.6674958540630183, 'f2_weighted': 0.659103113925878}\n",
      "(1317,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6741293532338308, 'f2_macro': 0.5976637318512521, 'f2_micro': 0.6741293532338308, 'f2_weighted': 0.6648936071711207}\n",
      "(1322,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6741293532338308, 'f2_macro': 0.5690509060279768, 'f2_micro': 0.6741293532338308, 'f2_weighted': 0.6662102889579999}\n",
      "(1346,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6699834162520729, 'f2_macro': 0.5648122194682588, 'f2_micro': 0.6699834162520729, 'f2_weighted': 0.6609181592045426}\n",
      "(1342,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.693200663349917, 'f2_macro': 0.5793261221669161, 'f2_micro': 0.693200663349917, 'f2_weighted': 0.6830639188519616}\n",
      "(1295,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.693200663349917, 'f2_macro': 0.5870118259100445, 'f2_micro': 0.693200663349917, 'f2_weighted': 0.6843917429256634}\n",
      "(1332,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6475953565505804, 'f2_macro': 0.5540362781706555, 'f2_micro': 0.6475953565505804, 'f2_weighted': 0.6400347780645526}\n",
      "mean accuracy: 0.6721393034825871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "#     clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "    lp_model = LabelSpreading(kernel='rbf',\n",
    "                              gamma=10, \n",
    "#                               n_neighbors=25,\n",
    "                              alpha=0.95,\n",
    "                              max_iter=50,\n",
    "                              n_jobs=6)\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    xsemi_tfidf_ngram = tfidf_vect_ngram.transform(np.concatenate([X_test, df_test[\"text\"]]))\n",
    "\n",
    "    # First training using only training set\n",
    "    ysemi_unlabeld = -np.ones((X_test.shape[0]+df_test.shape[0],))\n",
    "    lp_model.fit(sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram)).toarray(),\n",
    "                 np.concatenate((Y_train, ysemi_unlabeld)))\n",
    "    \n",
    "    ysemi = lp_model.transduction_[xtrain_tfidf_ngram.shape[0]:]\n",
    "    \n",
    "    # semi supervised stage\n",
    "    THRESHOLD = 0.8\n",
    "#     pred_entropies_semi = stats.distributions.entropy(lp_model.label_distributions_.T)[X_train.shape[0]:]\n",
    "#     mask = pred_entropies_semi < THRESHOLD\n",
    "\n",
    "    mask = np.amax(lp_model.label_distributions_[X_train.shape[0]:], axis=1) > THRESHOLD\n",
    "    \n",
    "    print(ysemi[mask].shape)\n",
    "\n",
    "    # Use SVC for last epoch\n",
    "    metrics = train_model(SVC(kernel='linear'),\n",
    "                          sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((Y_train, ysemi[mask])), \n",
    "                          xvalid_tfidf_ngram, Y_test)\n",
    "#             metrics = train_model(xgboost.XGBClassifier(max_depth=5, n_estimators=140),\n",
    "#                               sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "#                               np.concatenate((Y_train, ysemi[mask])), \n",
    "#                               xvalid_tfidf_ngram, Y_test)\n",
    "\n",
    "    metrics_list.append(metrics)\n",
    "    print(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "# print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "# print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "# print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.6\n",
    "0.6708\n",
    "\n",
    "0.55\n",
    "0.6693\n",
    "\n",
    "0.5\n",
    "0.6673\n",
    "\n",
    "0.7\n",
    "0.6713\n",
    "\n",
    "0.8\n",
    "0.6721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(961,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py:292: ConvergenceWarning: max_iter=50 was reached without convergence.\n",
      "  category=ConvergenceWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On training set: \n",
      "mean accuracy: 0.7363530778164924\n",
      "mean f2_macro: 0.6264230999234784\n",
      "mean f2_micro: 0.7363530778164924\n",
      "mean f2_weighted: 0.7280533353192284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION\n",
    "\n",
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "\n",
    "lp_model = LabelSpreading(kernel='rbf',\n",
    "                          gamma=10, \n",
    "#                         n_neighbors=25,\n",
    "                          alpha=0.95,\n",
    "                          max_iter=50,\n",
    "                          n_jobs=6)\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"text\"])\n",
    "xsemi_tfidf_ngram = tfidf_vect_ngram.transform(df_test[\"text\"])\n",
    "\n",
    "ysemi_unlabeld = -np.ones((df_test.shape[0],))\n",
    "lp_model.fit(sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram)).toarray(),\n",
    "             np.concatenate((df_noerror[\"category\"], ysemi_unlabeld)))\n",
    "\n",
    "# semi supervised stage\n",
    "# create labels\n",
    "ysemi = lp_model.transduction_[df_noerror.shape[0]:]\n",
    "THRESHOLD = 0.8\n",
    "mask = np.amax(lp_model.label_distributions_[df_noerror.shape[0]:], axis=1) > THRESHOLD\n",
    "\n",
    "print(ysemi[mask].shape)\n",
    "\n",
    "# Use SVC for last epoch\n",
    "final_clf = SVC(kernel='linear')\n",
    "metrics = train_model(final_clf,\n",
    "                  sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                  np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                  train_tfidf_ngram, df_noerror[\"category\"])\n",
    "\n",
    "metrics_list.append(metrics)\n",
    "\n",
    "print(\"On training set: \")\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., 4., 4., 4., 4., 4., 2., 2.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf.predict(xsemi_tfidf_ngram)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3826, 9)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test[\"category\"] = [int(i) for i in final_clf.predict(xsemi_tfidf_ngram)]\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext_1436.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2960,)\n",
      "(3079,)\n",
      "(2963,)\n",
      "(3019,)\n",
      "(2971,)\n",
      "(3030,)\n",
      "(2990,)\n",
      "(2994,)\n",
      "(2955,)\n",
      "(2977,)\n",
      "mean accuracy: 0.6745439469320066\n",
      "mean f2_macro: 0.5727358042809104\n",
      "mean f2_micro: 0.6745439469320065\n",
      "mean f2_weighted: 0.6646584349924545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "    clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    xsemi_tfidf_ngram = tfidf_vect_ngram.transform(np.concatenate([X_test, df_test[\"text\"]]))\n",
    "\n",
    "    # First training using only training set\n",
    "    _ = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "\n",
    "    # semi supervised stage\n",
    "    for epoch in range(epochs):\n",
    "        # create labels\n",
    "        prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "        THRESHOLD = 0.6\n",
    "        mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "        ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "        print(ysemi[mask].shape)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            # Use SVC for last epoch\n",
    "            metrics = train_model(SVC(kernel='linear'),\n",
    "                                  sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train, ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "#             metrics = train_model(xgboost.XGBClassifier(max_depth=5, n_estimators=140),\n",
    "#                               sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "#                               np.concatenate((Y_train, ysemi[mask])), \n",
    "#                               xvalid_tfidf_ngram, Y_test)\n",
    "        else:\n",
    "            metrics = train_model(clf,\n",
    "                                  sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train, ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_replicates=6, without using stem, threshold = 0.6\n",
    "0.6742, 0.5699\n",
    "\n",
    "title_replicates=6, using stem, threshold = 0.6\n",
    "0.6752, 0.5723\n",
    "\n",
    "using high conf for training set, title_replicates=6, using stem, threshold = 0.6\n",
    "0.6766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_replicates=10, using stem, threshold = 0.6\n",
    "0.6748\n",
    "\n",
    "high_conf for train, title_replicates=10, using stem, threshold = 0.6\n",
    "0.6690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_replicates=20, using stem, threshold = 0.6\n",
    "0.6727\n",
    "\n",
    "using high conf for training set, title_replicates=20, using stem, threshold = 0.6\n",
    "0.6717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_replicates=10, using stem\n",
    "threshold\n",
    "\n",
    "0.8\n",
    "0.6723\n",
    "\n",
    "0.6\n",
    "0.6759\n",
    "\n",
    "0.5\n",
    "0.6722\n",
    "\n",
    "0.0\n",
    "0.6664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3487,)\n",
      "(3526,)\n",
      "(3516,)\n",
      "(3480,)\n",
      "(3412,)\n",
      "(3465,)\n",
      "(3445,)\n",
      "(3442,)\n",
      "(3426,)\n",
      "(3529,)\n",
      "mean accuracy: 0.6713101160862354\n",
      "mean f2_macro: 0.565372718338484\n",
      "mean f2_micro: 0.6713101160862354\n",
      "mean f2_weighted: 0.6593736716144306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use high confident data as extra training data\n",
    "# Use high confident training data as training data\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "#     clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "    clf = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    xsemi_tfidf_ngram = tfidf_vect_ngram.transform(np.concatenate([X_test, df_test[\"text\"]]))\n",
    "\n",
    "    # First training using only training set\n",
    "    _ = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "\n",
    "    # semi supervised stage\n",
    "    for epoch in range(epochs):\n",
    "        # create labels\n",
    "        prob_train = clf.predict_proba(xtrain_tfidf_ngram)\n",
    "        prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "        THRESHOLD = 0.6\n",
    "        mask_train = np.amax(prob_train, axis=1) > THRESHOLD\n",
    "        mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "        ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "        print(ysemi[mask].shape)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            # Use SVC for last epoch\n",
    "            metrics = train_model(SVC(kernel='linear'),\n",
    "                                  sp.vstack((xtrain_tfidf_ngram[mask_train], xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train[mask_train], ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "#             metrics = train_model(xgboost.XGBClassifier(max_depth=5, n_estimators=140),\n",
    "#                               sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "#                               np.concatenate((Y_train, ysemi[mask])), \n",
    "#                               xvalid_tfidf_ngram, Y_test)\n",
    "        else:\n",
    "            metrics = train_model(clf,\n",
    "                                  sp.vstack((xtrain_tfidf_ngram[mask_train], xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train[mask_train], ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# warn, epoch=1\n",
    "\n",
    "# RF + SVC, 0.6\n",
    "# 0.6749  # however does NOT improve final submission\n",
    "\n",
    "# RF + SVC, 0.7\n",
    "# 0.6753\n",
    "\n",
    "# RF + SVC, 0.8\n",
    "# 0.6744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.664179104477612, 'f2_macro': 0.5598825270772545, 'f2_micro': 0.664179104477612, 'f2_weighted': 0.6544175354132418}\n",
      "{'accuracy': 0.6583747927031509, 'f2_macro': 0.5340564591583843, 'f2_micro': 0.6583747927031509, 'f2_weighted': 0.6466752674710315}\n",
      "{'accuracy': 0.6733001658374793, 'f2_macro': 0.5559213384390573, 'f2_micro': 0.6733001658374793, 'f2_weighted': 0.6616446004646143}\n",
      "{'accuracy': 0.6633499170812603, 'f2_macro': 0.5676067079626416, 'f2_micro': 0.6633499170812603, 'f2_weighted': 0.6536294834846148}\n",
      "{'accuracy': 0.6708126036484245, 'f2_macro': 0.592227189547627, 'f2_micro': 0.6708126036484245, 'f2_weighted': 0.6609985405874532}\n",
      "{'accuracy': 0.6666666666666666, 'f2_macro': 0.5584270955120385, 'f2_micro': 0.6666666666666666, 'f2_weighted': 0.6580148476825817}\n",
      "{'accuracy': 0.6708126036484245, 'f2_macro': 0.5573280800842287, 'f2_micro': 0.6708126036484245, 'f2_weighted': 0.6612956322890229}\n",
      "{'accuracy': 0.6898839137645107, 'f2_macro': 0.5696537659866285, 'f2_micro': 0.6898839137645106, 'f2_weighted': 0.6792766932824722}\n",
      "{'accuracy': 0.6940298507462687, 'f2_macro': 0.5797602836897815, 'f2_micro': 0.6940298507462687, 'f2_weighted': 0.6841859974585657}\n",
      "{'accuracy': 0.6492537313432836, 'f2_macro': 0.5530656053569377, 'f2_micro': 0.6492537313432836, 'f2_weighted': 0.6415316816190662}\n",
      "mean accuracy: 0.6700663349917082\n",
      "mean f2_macro: 0.562792905281458\n",
      "mean f2_micro: 0.6700663349917081\n",
      "mean f2_weighted: 0.6601670279752664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a single classifier\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "#     clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, n_estimators=140)\n",
    "    clf = SVC(kernel='linear')\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    metrics = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "    \n",
    "    print(metrics)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "On training set: \n",
      "mean accuracy: 0.7353575576572092\n",
      "mean f2_macro: 0.6255968857777343\n",
      "mean f2_micro: 0.7353575576572092\n",
      "mean f2_weighted: 0.7268130099549321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION\n",
    "\n",
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "\n",
    "# clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"text\"])\n",
    "xsemi_tfidf_ngram = tfidf_vect_ngram.transform(df_test[\"text\"])\n",
    "\n",
    "# First training using only training set\n",
    "_ = train_model(clf, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "\n",
    "# semi supervised stage\n",
    "for epoch in range(epochs):\n",
    "    # create labels\n",
    "    prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "    THRESHOLD = 0.6\n",
    "    mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "    ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "    print(ysemi[mask].shape)\n",
    "\n",
    "    if epoch == epochs-1:\n",
    "        # Use SVC for last epoch\n",
    "        final_clf = SVC(kernel='linear')\n",
    "        metrics = train_model(final_clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "    else:\n",
    "        metrics = train_model(clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "metrics_list.append(metrics)\n",
    "\n",
    "print(\"On training set: \")\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_list.append(final_clf.predict(xsemi_tfidf_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376,)\n",
      "On training set: \n",
      "mean accuracy: 0.73353243736519\n",
      "mean f2_macro: 0.6236483792314528\n",
      "mean f2_micro: 0.73353243736519\n",
      "mean f2_weighted: 0.7249394600144747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION\n",
    "\n",
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "\n",
    "# clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"text\"])\n",
    "xsemi_tfidf_ngram = tfidf_vect_ngram.transform(df_test[\"text\"])\n",
    "\n",
    "# First training using only training set\n",
    "_ = train_model(clf, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "\n",
    "# semi supervised stage\n",
    "for epoch in range(epochs):\n",
    "    # create labels\n",
    "    prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "    THRESHOLD = 0.6\n",
    "    mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "    ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "    print(ysemi[mask].shape)\n",
    "\n",
    "    if epoch == epochs-1:\n",
    "        # Use SVC for last epoch\n",
    "        final_clf = SVC(kernel='linear')\n",
    "        metrics = train_model(final_clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "    else:\n",
    "        metrics = train_model(clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "metrics_list.append(metrics)\n",
    "\n",
    "print(\"On training set: \")\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_list.append(final_clf.predict(xsemi_tfidf_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2391,)\n",
      "On training set: \n",
      "mean accuracy: 0.734362037497926\n",
      "mean f2_macro: 0.6248781419612619\n",
      "mean f2_micro: 0.734362037497926\n",
      "mean f2_weighted: 0.7259651814405409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION\n",
    "\n",
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "\n",
    "# clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"text\"])\n",
    "xsemi_tfidf_ngram = tfidf_vect_ngram.transform(df_test[\"text\"])\n",
    "\n",
    "# First training using only training set\n",
    "_ = train_model(clf, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "\n",
    "# semi supervised stage\n",
    "for epoch in range(epochs):\n",
    "    # create labels\n",
    "    prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "    THRESHOLD = 0.6\n",
    "    mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "    ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "    print(ysemi[mask].shape)\n",
    "\n",
    "    if epoch == epochs-1:\n",
    "        # Use SVC for last epoch\n",
    "        final_clf = SVC(kernel='linear')\n",
    "        metrics = train_model(final_clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "    else:\n",
    "        metrics = train_model(clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "metrics_list.append(metrics)\n",
    "\n",
    "print(\"On training set: \")\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuse_results(predictions_list, xgb_offset=1):\n",
    "    final_predictions = []\n",
    "    percentage = 0\n",
    "    for preds in zip(*predictions_list):\n",
    "        sr_count = pd.Series(preds).value_counts()\n",
    "        if sr_count.iloc[0] >= 2 and sr_count.index[0] != preds[1]:\n",
    "            final_predictions.append(sr_count.index[0])  # the most commonly predicted class\n",
    "            percentage +=1\n",
    "        else:\n",
    "            final_predictions.append(preds[1])  # use result of xgb\n",
    "    \n",
    "    print(percentage)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "final_predictions = fuse_results(predictions_list, xgb_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test[\"category\"] = final_predictions\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test[\"category\"] = final_clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors:  0.6826462128475551\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(LogisticRegression(penalty='l2', C=1.25, solver='lbfgs'), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors:  0.6586768935762224\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(MultinomialNB(alpha=0.8), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.6711409395973155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=160), xtrain_tfidf_ngram.tocsc(), Y_train, xvalid_tfidf_ngram.tocsc(), Y_test)\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors:  0.653910149750416\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(RandomForestClassifier(n_estimators=200, criterion='entropy'), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"RF, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems that by filtering out those in the train_warn_list, the accuracy obtained is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission Using Different Mothods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=3000)\n",
    "tfidf_vect_ngram.fit(df_noerror[\"data\"])\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"data\"])\n",
    "test_tfidf_ngram =  tfidf_vect_ngram.transform(df_test[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR on training set  0.7286509307234695\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(penalty='l2', C=1.25, solver='lbfgs')\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"LR on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier on Ngram Level TF IDF Vectors\n",
    "classifier =  MultinomialNB()\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"NB on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB on training set  0.8842832469775475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "classifier = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"XGB on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF on training set  0.998272884283247\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"RF on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_data[\"category\"] = classifier.predict(test_tfidf_ngram.tocsc())\n",
    "df_test[\"category\"] = classifier.predict(test_tfidf_ngram)\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update prediction by title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('prediction.csv')\n",
    "df_pred_fulltext = pd.read_csv('prediction_fulltext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updated_id = []\n",
    "updated_cat = []\n",
    "num_diff = 0\n",
    "for row in df_pred.itertuples():\n",
    "    updated_id.append(row.article_id)\n",
    "    cat_1 = row.category\n",
    "    _ls = df_pred_fulltext[df_pred_fulltext.article_id==row.article_id].category.values\n",
    "   \n",
    "    if _ls.size > 0:  # if calculated in fulltext prediction\n",
    "        cat_2 = _ls[0]\n",
    "        # print(cat_1, cat_2)\n",
    "        updated_cat.append(cat_2)\n",
    "        if cat_1 != cat_2:\n",
    "            num_diff += 1\n",
    "    else:\n",
    "        updated_cat.append(cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_updated = pd.DataFrame({\n",
    "    'article_id': updated_id,\n",
    "    'category': updated_cat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_updated.to_csv('prediction_updated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
