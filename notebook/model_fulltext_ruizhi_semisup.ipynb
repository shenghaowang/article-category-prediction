{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, fbeta_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"input/train_v2.csv\")\n",
    "test_data = pd.read_csv(\"input/test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Forex - Pound drops to one-month lows against ...</td>\n",
       "      <td>http://www.nasdaq.com/article/forex-pound-drop...</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>www.nasdaq.com</td>\n",
       "      <td>1.390000e+12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0           1  Forex - Pound drops to one-month lows against ...   \n",
       "\n",
       "                                                 url publisher  \\\n",
       "0  http://www.nasdaq.com/article/forex-pound-drop...    NASDAQ   \n",
       "\n",
       "         hostname     timestamp  category  \n",
       "0  www.nasdaq.com  1.390000e+12         4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warn_filename_train = 'raw_train/warn.txt'\n",
    "error_filename_train = 'raw_train/error.txt'\n",
    "warn_filename_test = 'raw_test/warn.txt'\n",
    "error_filename_test = 'raw_test/error.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_warn_or_error_list(filename):\n",
    "    with open(filename, errors='ignore') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    _list = []\n",
    "    for line in lines:\n",
    "        _list.append(line.split(',')[0])\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_warn_list = read_warn_or_error_list(warn_filename_train)\n",
    "train_error_list = read_warn_or_error_list(error_filename_train)\n",
    "test_warn_list = read_warn_or_error_list(warn_filename_test)\n",
    "test_error_list = read_warn_or_error_list(error_filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('train_full.txt', 'w',errors='ignore') as myfile:\n",
    "#     for i in range(len(train_data['category'])):\n",
    "#         filename='raw_train/'+str(i+1)+'.txt'\n",
    "#         if os.path.exists(filename):\n",
    "#             with open(filename, 'r',errors='ignore') as data_file:\n",
    "#                 data=data_file.read().replace('\\n', '')\n",
    "#             myfile.write(train_data['title'][i]+\" \"+data+ '\\n')\n",
    "#         else:\n",
    "#             myfile.write(train_data['title'][i]+ '\\n')\n",
    "\n",
    "# #v2: remove numbers and punctuations\n",
    "# whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "# with open('train_full_clean.txt', 'w',errors='ignore') as myfile:\n",
    "#     for i in range(len(train_data['category'])):\n",
    "#         filename='raw_train/'+str(i+1)+'.txt'\n",
    "#         title=train_data['title'][i]\n",
    "#         title = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", title)\n",
    "#         title = ''.join(filter(whitelist.__contains__, title)).lower()\n",
    "#         title = ' '.join(title.split())\n",
    "#         if os.path.exists(filename):\n",
    "#             with open(filename, 'r',errors='ignore') as data_file:\n",
    "#                 data=data_file.read().replace('\\n', '')\n",
    "#                 data = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", data)\n",
    "#                 data = ''.join(filter(whitelist.__contains__, data)).lower()\n",
    "#                 data=' '.join(data.split())\n",
    "#             myfile.write(title+\" \"+data+ '\\n')\n",
    "#         else:\n",
    "#             myfile.write(title+ '\\n')\n",
    "\n",
    "# with open('test_full.txt', 'w',errors='ignore') as myfile:\n",
    "#     for i in range(len(test_data['title'])):\n",
    "#         filename='raw_test/'+str(i+1)+'.txt'\n",
    "#         if os.path.exists(filename):\n",
    "#             with open(filename, 'r',errors='ignore') as data_file:\n",
    "#                 data=data_file.read().replace('\\n', '')\n",
    "#             myfile.write(test_data['title'][i]+\" \"+data+ '\\n')\n",
    "#         else:\n",
    "#             myfile.write(test_data['title'][i]+ '\\n')\n",
    "\n",
    "# #v2 remove numbers and punctuations\n",
    "# whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "# with open('test_full_clean.txt', 'w',errors='ignore') as myfile:\n",
    "#     for i in range(len(test_data['title'])):\n",
    "#         filename='raw_test/'+str(i+1)+'.txt'\n",
    "#         title=test_data['title'][i]\n",
    "#         title = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", title)\n",
    "#         title = ''.join(filter(whitelist.__contains__, title)).lower()\n",
    "#         title = ' '.join(title.split())\n",
    "#         if os.path.exists(filename):\n",
    "#             with open(filename, 'r',errors='ignore') as data_file:\n",
    "#                 data=data_file.read().replace('\\n', '')\n",
    "#                 data = re.sub(r\"[,:.;@#\\?\\-\\%\\/!&$]+\\ *\", \" \", data)\n",
    "#                 data = ''.join(filter(whitelist.__contains__, data)).lower()\n",
    "#                 data=' '.join(data.split())\n",
    "#             myfile.write(title +\" \"+data+ '\\n')\n",
    "#         else:\n",
    "#             myfile.write(title + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(sent, stemmer_type='porter'):\n",
    "    '''\n",
    "    stemmer_type can be porter, lancaster, or snowball\n",
    "    '''\n",
    "    if stemmer_type == 'porter':\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer_type == 'lancaster':\n",
    "        stemmer = LancasterStemmer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    words = gensim.utils.simple_preprocess(sent)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forex - Pound drops to one-month lows against ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  article_id  category\n",
       "0  Forex - Pound drops to one-month lows against ...           1         4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainf='train_full_clean.txt'\n",
    "trainf='train_full.txt'\n",
    "#X = pd.read_csv(trainf,header=None,sep=\",\",names=['data'])\n",
    "with open(trainf) as f:\n",
    "    X = f.read().splitlines()\n",
    "\n",
    "df = pd.DataFrame({\"data\": X})\n",
    "df = df.assign(article_id=train_data.article_id, category=train_data.category)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White House plays down speedy role for US natu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  article_id\n",
       "0  White House plays down speedy role for US natu...           1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainf='train_full_clean.txt'\n",
    "trainf='test_full.txt'\n",
    "#X = pd.read_csv(trainf,header=None,sep=\",\",names=['data'])\n",
    "with open(trainf) as f:\n",
    "    X = f.read().splitlines()\n",
    "\n",
    "df_test = pd.DataFrame({\"data\": X})\n",
    "df_test = df_test.assign(article_id=test_data.article_id)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_noerror = df[~df.article_id.isin(train_warn_list)]\n",
    "# df_noerror = df[~df.article_id.isin(train_error_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df_test[~df_test.article_id.isin(test_warn_list)]\n",
    "# df_test = df_test[~df_test.article_id.isin(test_error_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# perform stemming\n",
    "df_noerror.loc[:, \"data\"] = df_noerror[\"data\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.loc[:, \"data\"] = df_test[\"data\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5211, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noerror.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3344, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forex pound drop to one month low against euro...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  article_id  category\n",
       "0  forex pound drop to one month low against euro...           1         4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noerror.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white hous play down speedi role for us natur ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  article_id\n",
       "0  white hous play down speedi role for us natur ...           1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a25bf39b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFJjRkCSGvtfuCY+RPx9pYcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a256d9ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_noerror.category.value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label_train, feature_vector_valid, label_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label_train)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return {'accuracy': accuracy_score(label_valid, predictions), \n",
    "            # 'f2_seperate': fbeta_score(Y_test, Y_predict, average=None, beta=2),\n",
    "            'f2_macro': fbeta_score(label_valid, predictions, average='macro', beta=2),\n",
    "            'f2_micro': fbeta_score(label_valid, predictions, average='micro', beta=2),\n",
    "            'f2_weighted': fbeta_score(label_valid, predictions, average='weighted', beta=2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_data(train_data):\n",
    "    X = train_data[\"data\"].values\n",
    "    y = train_data[\"category\"].values\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        yield X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',\n",
    "                                   # token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(1,2), \n",
    "                                   max_df=1.0,\n",
    "                                   min_df=1,\n",
    "                                   max_features=4000)\n",
    "\n",
    "tfidf_vect_ngram.fit(df_noerror[\"data\"].tolist() + df_test[\"data\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1295,)\n",
      "(1270,)\n",
      "(1265,)\n",
      "(1301,)\n",
      "(1299,)\n",
      "mean accuracy: 0.6744007670182167\n",
      "mean f2_macro: 0.5749448797008796\n",
      "mean f2_micro: 0.6744007670182167\n",
      "mean f2_weighted: 0.6665107585495939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "    clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, n_estimators=140)\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    xsemi_tfidf_ngram = tfidf_vect_ngram.transform(np.concatenate([X_test, df_test[\"data\"]]))\n",
    "\n",
    "    # First training using only training set\n",
    "    _ = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "\n",
    "    # semi supervised stage\n",
    "    for epoch in range(epochs):\n",
    "        # create labels\n",
    "        prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "        THRESHOLD = 0.8\n",
    "        mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "        ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "        print(ysemi[mask].shape)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            # Use SVC for last epoch\n",
    "            metrics = train_model(SVC(kernel='linear'),\n",
    "                                  sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train, ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "#             metrics = train_model(xgboost.XGBClassifier(max_depth=5, n_estimators=140),\n",
    "#                               sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "#                               np.concatenate((Y_train, ysemi[mask])), \n",
    "#                               xvalid_tfidf_ngram, Y_test)\n",
    "        else:\n",
    "            metrics = train_model(clf,\n",
    "                                  sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train, ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# warn, epoch=1\n",
    "\n",
    "# RF + SVC, 0.6\n",
    "# 0.6749  # however does NOT improve final submission\n",
    "\n",
    "# RF + SVC, 0.7\n",
    "# 0.6753\n",
    "\n",
    "# RF + SVC, 0.8\n",
    "# 0.6744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.6730584851390221\n",
      "mean f2_macro: 0.5684627179694651\n",
      "mean f2_micro: 0.6730584851390221\n",
      "mean f2_weighted: 0.6644311629164557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a single classifier\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "#     clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, n_estimators=140)\n",
    "    clf = SVC(kernel='linear')\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    metrics = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warn\n",
    "\n",
    "SVC\n",
    "0.6730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1948,)\n",
      "On training set: \n",
      "mean accuracy: 0.7336403761274227\n",
      "mean f2_macro: 0.6225990408674156\n",
      "mean f2_micro: 0.7336403761274227\n",
      "mean f2_weighted: 0.7252806510868367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION\n",
    "\n",
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "\n",
    "# clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"data\"])\n",
    "xsemi_tfidf_ngram = tfidf_vect_ngram.transform(df_test[\"data\"])\n",
    "\n",
    "# First training using only training set\n",
    "_ = train_model(clf, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "\n",
    "# semi supervised stage\n",
    "for epoch in range(epochs):\n",
    "    # create labels\n",
    "    prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "    THRESHOLD = 0.6\n",
    "    mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "    ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "    print(ysemi[mask].shape)\n",
    "\n",
    "    if epoch == epochs-1:\n",
    "        # Use SVC for last epoch\n",
    "        final_clf = SVC(kernel='linear')\n",
    "        metrics = train_model(final_clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "    else:\n",
    "        metrics = train_model(clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "metrics_list.append(metrics)\n",
    "\n",
    "print(\"On training set: \")\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"category\"] = final_clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors:  0.6826462128475551\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(LogisticRegression(penalty='l2', C=1.25, solver='lbfgs'), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors:  0.6586768935762224\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(MultinomialNB(alpha=0.8), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.6711409395973155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=160), xtrain_tfidf_ngram.tocsc(), Y_train, xvalid_tfidf_ngram.tocsc(), Y_test)\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors:  0.653910149750416\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(RandomForestClassifier(n_estimators=200, criterion='entropy'), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"RF, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems that by filtering out those in the train_warn_list, the accuracy obtained is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission Using Different Mothods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=3000)\n",
    "tfidf_vect_ngram.fit(df_noerror[\"data\"])\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"data\"])\n",
    "test_tfidf_ngram =  tfidf_vect_ngram.transform(df_test[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR on training set  0.7286509307234695\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(penalty='l2', C=1.25, solver='lbfgs')\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"LR on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier on Ngram Level TF IDF Vectors\n",
    "classifier =  MultinomialNB()\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"NB on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB on training set  0.8842832469775475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "classifier = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"XGB on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF on training set  0.998272884283247\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"RF on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_data[\"category\"] = classifier.predict(test_tfidf_ngram.tocsc())\n",
    "df_test[\"category\"] = classifier.predict(test_tfidf_ngram)\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update prediction by title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('prediction.csv')\n",
    "df_pred_fulltext = pd.read_csv('prediction_fulltext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updated_id = []\n",
    "updated_cat = []\n",
    "num_diff = 0\n",
    "for row in df_pred.itertuples():\n",
    "    updated_id.append(row.article_id)\n",
    "    cat_1 = row.category\n",
    "    _ls = df_pred_fulltext[df_pred_fulltext.article_id==row.article_id].category.values\n",
    "   \n",
    "    if _ls.size > 0:  # if calculated in fulltext prediction\n",
    "        cat_2 = _ls[0]\n",
    "        # print(cat_1, cat_2)\n",
    "        updated_cat.append(cat_2)\n",
    "        if cat_1 != cat_2:\n",
    "            num_diff += 1\n",
    "    else:\n",
    "        updated_cat.append(cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_updated = pd.DataFrame({\n",
    "    'article_id': updated_id,\n",
    "    'category': updated_cat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_updated.to_csv('prediction_updated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
