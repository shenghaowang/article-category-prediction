{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, fbeta_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"input/train_v2.csv\")\n",
    "test_data = pd.read_csv(\"input/test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Forex - Pound drops to one-month lows against ...</td>\n",
       "      <td>http://www.nasdaq.com/article/forex-pound-drop...</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>www.nasdaq.com</td>\n",
       "      <td>1.390000e+12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0           1  Forex - Pound drops to one-month lows against ...   \n",
       "\n",
       "                                                 url publisher  \\\n",
       "0  http://www.nasdaq.com/article/forex-pound-drop...    NASDAQ   \n",
       "\n",
       "         hostname     timestamp  category  \n",
       "0  www.nasdaq.com  1.390000e+12         4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_warn_or_error_list(filename):\n",
    "    with open(filename, errors='ignore') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    _list = []\n",
    "    for line in lines:\n",
    "        _list.append(line.split(',')[0])\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success_train = 'raw_train_new/success.txt'\n",
    "success_test = 'raw_test_new/success.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success_train_list = read_warn_or_error_list(success_train)\n",
    "success_test_list = read_warn_or_error_list(success_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(success_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "798"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(success_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fulltext_list = []\n",
    "for i in train_data['article_id']:\n",
    "    if str(i) in success_train_list:\n",
    "        filename='raw_train_new/'+str(i)+'.txt'\n",
    "        with open(filename, 'r',errors='ignore') as data_file:\n",
    "            train_fulltext_list.append(train_data[train_data.article_id == i].title.iloc[0]+ ' ' + data_file.read().replace('\\n', ''))\n",
    "    else:\n",
    "        train_fulltext_list.append(train_data[train_data.article_id == i].title.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fulltext_list = []\n",
    "for i in test_data['article_id']:\n",
    "    if str(i) in success_test_list:\n",
    "        filename='raw_test_new/'+str(i)+'.txt'\n",
    "        with open(filename, 'r',errors='ignore') as data_file:\n",
    "            test_fulltext_list.append(test_data[test_data.article_id == i].title.iloc[0]+ ' ' + data_file.read().replace('\\n', ''))\n",
    "    else:\n",
    "        test_fulltext_list.append(test_data[test_data.article_id == i].title.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6027"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_fulltext_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3826"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_fulltext_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(sent, stemmer_type='porter'):\n",
    "    '''\n",
    "    stemmer_type can be porter, lancaster, or snowball\n",
    "    '''\n",
    "    if stemmer_type == 'porter':\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer_type == 'lancaster':\n",
    "        stemmer = LancasterStemmer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    words = gensim.utils.simple_preprocess(sent)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forex - Pound drops to one-month lows against ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  article_id  category\n",
       "0  Forex - Pound drops to one-month lows against ...           1         4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #trainf='train_full_clean.txt'\n",
    "# trainf='train_full.txt'\n",
    "# #X = pd.read_csv(trainf,header=None,sep=\",\",names=['data'])\n",
    "# with open(trainf) as f:\n",
    "#     X = f.read().splitlines()\n",
    "\n",
    "df = pd.DataFrame({\"data\": train_fulltext_list})\n",
    "df = df.assign(article_id=train_data.article_id, category=train_data.category)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White House plays down speedy role for US natu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  article_id\n",
       "0  White House plays down speedy role for US natu...           1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #trainf='train_full_clean.txt'\n",
    "# trainf='test_full.txt'\n",
    "# #X = pd.read_csv(trainf,header=None,sep=\",\",names=['data'])\n",
    "# with open(trainf) as f:\n",
    "#     X = f.read().splitlines()\n",
    "\n",
    "df_test = pd.DataFrame({\"data\": test_fulltext_list})\n",
    "df_test = df_test.assign(article_id=test_data.article_id)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_noerror = df[df.article_id.isin(success_train_list)]\n",
    "df_noerror = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test = df_test[df_test.article_id.isin(success_train_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform stemming\n",
    "df_noerror.loc[:, \"data\"] = df_noerror[\"data\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.loc[:, \"data\"] = df_test[\"data\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6027, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noerror.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3826, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forex pound drop to one month low against euro</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             data  article_id  category\n",
       "0  forex pound drop to one month low against euro           1         4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noerror.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white hous play down speedi role for us natur ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  article_id\n",
       "0  white hous play down speedi role for us natur ...           1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a19f39b70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADudJREFUeJzt3X+s3XV9x/HnS1DCphgMd01tq8WsTgtudTSVxWXBmY1O\nlxWTzZQ/bLM46kJ1mJhl4P6Qf5rwx9SMRNjqZJTF2XT+CM0GGqxuxiwIF9ZY2trRCYzelHKdy6rR\n4Fre++N+up5cWu7PnlP4PB/Jyfmc9/fHeZ9voK/z/XVuqgpJUp9eMeoGJEmjYwhIUscMAUnqmCEg\nSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOnbhqBuYyWWXXVYrV64cdRuS9JLyyCOP/KCqxmaa77wP\ngZUrVzI+Pj7qNiTpJSXJU7OZz8NBktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOGgCR1zBCQpI6d\n9zeLLdTKm/9p1C0A8ORt7x11C5L0Au4JSFLHDAFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNA\nkjpmCEhSxwwBSeqYISBJHTMEJKljhoAkdWzGEEiyIsk3kxxIsj/JTa1+a5KJJHvb4z0Dy9yS5HCS\nQ0muHahflWRfm3Z7kpybjyVJmo3Z/JT0CeBjVfVoktcAjyR5oE37dFX9xeDMSVYDG4ErgNcDX0/y\n5qo6CdwJ3AB8B7gPWA/cvzgfRZI0VzPuCVTV0ap6tI1/BBwElr3IIhuAnVX1XFU9ARwG1iVZClxS\nVQ9WVQH3ANct+BNIkuZtTucEkqwE3s7UN3mAjyT5bpK7klzaasuApwcWO9Jqy9p4el2SNCKzDoEk\nrwa+BHy0qo4zdWjnTcAa4CjwycVqKsmWJONJxicnJxdrtZKkaWYVAkleyVQAfL6qvgxQVceq6mRV\nPQ98FljXZp8AVgwsvrzVJtp4ev0Fqmp7Va2tqrVjY2Nz+TySpDmYzdVBAT4HHKyqTw3Ulw7M9j7g\nsTbeDWxMclGSy4FVwENVdRQ4nuTqts5NwL2L9DkkSfMwm6uD3gl8ANiXZG+rfRy4PskaoIAngQ8B\nVNX+JLuAA0xdWbS1XRkEcCNwN3AxU1cFeWWQJI3QjCFQVd8GznQ9/30vssw2YNsZ6uPAlXNpUJJ0\n7njHsCR1zBCQpI4ZApLUMUNAkjpmCEhSxwwBSeqYISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6\nZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOG\ngCR1zBCQpI4ZApLUMUNAkjpmCEhSx2YMgSQrknwzyYEk+5Pc1OqvS/JAksfb86UDy9yS5HCSQ0mu\nHahflWRfm3Z7kpybjyVJmo3Z7AmcAD5WVauBq4GtSVYDNwN7qmoVsKe9pk3bCFwBrAfuSHJBW9ed\nwA3AqvZYv4ifRZI0RzOGQFUdrapH2/hHwEFgGbAB2NFm2wFc18YbgJ1V9VxVPQEcBtYlWQpcUlUP\nVlUB9wwsI0kagTmdE0iyEng78B1gSVUdbZOeAZa08TLg6YHFjrTasjaeXj/T+2xJMp5kfHJyci4t\nSpLmYNYhkOTVwJeAj1bV8cFp7Zt9LVZTVbW9qtZW1dqxsbHFWq0kaZpZhUCSVzIVAJ+vqi+38rF2\niIf2/GyrTwArBhZf3moTbTy9LkkakdlcHRTgc8DBqvrUwKTdwOY23gzcO1DfmOSiJJczdQL4oXbo\n6HiSq9s6Nw0sI0kagQtnMc87gQ8A+5LsbbWPA7cBu5J8EHgKeD9AVe1Psgs4wNSVRVur6mRb7kbg\nbuBi4P72kCSNyIwhUFXfBs52Pf+7z7LMNmDbGerjwJVzaVCSdO54x7AkdcwQkKSOGQKS1DFDQJI6\nZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOG\ngCR1bDZ/XlIvF7e+dtQdTLn1f0bdgaTGPQFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNAkjpm\nCEhSxwwBSeqYISBJHTMEJKljM4ZAkruSPJvksYHarUkmkuxtj/cMTLslyeEkh5JcO1C/Ksm+Nu32\nJFn8jyNJmovZ7AncDaw/Q/3TVbWmPe4DSLIa2Ahc0Za5I8kFbf47gRuAVe1xpnVKkoZoxhCoqm8B\nP5zl+jYAO6vquap6AjgMrEuyFLikqh6sqgLuAa6bb9OSpMWxkHMCH0ny3Xa46NJWWwY8PTDPkVZb\n1sbT62eUZEuS8STjk5OTC2hRkvRi5hsCdwJvAtYAR4FPLlpHQFVtr6q1VbV2bGxsMVctSRowrxCo\nqmNVdbKqngc+C6xrkyaAFQOzLm+1iTaeXpckjdC8QqAd4z/lfcCpK4d2AxuTXJTkcqZOAD9UVUeB\n40mublcFbQLuXUDfkqRFMOOfl0zyBeAa4LIkR4BPANckWQMU8CTwIYCq2p9kF3AAOAFsraqTbVU3\nMnWl0cXA/e0hSRqhGUOgqq4/Q/lzLzL/NmDbGerjwJVz6k6SdE55x7AkdcwQkKSOGQKS1DFDQJI6\nZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOG\ngCR1zBCQpI4ZApLUMUNAkjpmCEhSxwwBSeqYISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghI\nUsdmDIEkdyV5NsljA7XXJXkgyePt+dKBabckOZzkUJJrB+pXJdnXpt2eJIv/cSRJczGbPYG7gfXT\najcDe6pqFbCnvSbJamAjcEVb5o4kF7Rl7gRuAFa1x/R1SpKGbMYQqKpvAT+cVt4A7GjjHcB1A/Wd\nVfVcVT0BHAbWJVkKXFJVD1ZVAfcMLCNJGpH5nhNYUlVH2/gZYEkbLwOeHpjvSKsta+Pp9TNKsiXJ\neJLxycnJebYoSZrJgk8Mt2/2tQi9DK5ze1Wtraq1Y2Nji7lqSdKA+YbAsXaIh/b8bKtPACsG5lve\nahNtPL0uSRqh+YbAbmBzG28G7h2ob0xyUZLLmToB/FA7dHQ8ydXtqqBNA8tIkkbkwplmSPIF4Brg\nsiRHgE8AtwG7knwQeAp4P0BV7U+yCzgAnAC2VtXJtqobmbrS6GLg/vaQJI3QjCFQVdefZdK7zzL/\nNmDbGerjwJVz6k6SdE55x7AkdcwQkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR0zBCSpY4aAJHXM\nEJCkjs34sxHSy9Hbdrxt1C0AsG/zvlG3oM65JyBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghI\nUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWMLCoEk\nTybZl2RvkvFWe12SB5I83p4vHZj/liSHkxxKcu1Cm5ckLcxi7Am8q6rWVNXa9vpmYE9VrQL2tNck\nWQ1sBK4A1gN3JLlgEd5fkjRP5+Jw0AZgRxvvAK4bqO+squeq6gngMLDuHLy/JGmWFhoCBXw9ySNJ\ntrTakqo62sbPAEvaeBnw9MCyR1rtBZJsSTKeZHxycnKBLUqSzubCBS7/61U1keQXgAeSfG9wYlVV\nkprrSqtqO7AdYO3atXNeXpI0OwvaE6iqifb8LPAVpg7vHEuyFKA9P9tmnwBWDCy+vNUkSSMy7xBI\n8vNJXnNqDPw28BiwG9jcZtsM3NvGu4GNSS5KcjmwCnhovu8vSVq4hRwOWgJ8Jcmp9fx9VX01ycPA\nriQfBJ4C3g9QVfuT7AIOACeArVV1ckHdS1qwg29566hbAOCt3zs46ha6NO8QqKrvA79yhvp/Ae8+\nyzLbgG3zfU9J0uLyjmFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUsYX+bIQkvWx85o+/MeoWANj6\nV785tPdyT0CSOmYISFLHDAFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNAkjpmCEhSxwwBSeqY\nISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR0zBCSpY4aAJHVs6CGQZH2S\nQ0kOJ7l52O8vSTptqCGQ5ALgM8DvAKuB65OsHmYPkqTThr0nsA44XFXfr6qfATuBDUPuQZLUpKqG\n92bJ7wPrq+qP2usPAO+oqg9Pm28LsKW9/CXg0NCaPLPLgB+MuIfzhdviNLfFaW6L086XbfHGqhqb\naaYLh9HJXFXVdmD7qPs4Jcl4Va0ddR/nA7fFaW6L09wWp73UtsWwDwdNACsGXi9vNUnSCAw7BB4G\nViW5PMmrgI3A7iH3IElqhno4qKpOJPkw8DXgAuCuqto/zB7m6bw5NHUecFuc5rY4zW1x2ktqWwz1\nxLAk6fziHcOS1DFDQJI6ZghIUscMAUnq2Hl5s9j5Jsk9VbVp1H2MQpK3AMuA71TVjwfq66vqq6Pr\nbPjattjA1PaAqXtcdlfVwdF1NRpJ1gFVVQ+33/9aD3yvqu4bcWuaI68OmibJ9PsWArwL+AZAVf3e\n0JsakSR/AmwFDgJrgJuq6t427dGq+tVR9jdMSf4MuJ6p37s60srLmbrXZWdV3Taq3oYtySeY+hHI\nC4EHgHcA3wR+C/haVW0bYXvnjSR/WFV/O+o+ZmIITJPkUeAA8DdAMRUCX2Dqf3aq6l9G191wJdkH\n/FpV/TjJSuCLwN9V1V8m+beqevtIGxyiJP8OXFFV/zut/ipgf1WtGk1nw9f+u1gDXAQ8AyyvquNJ\nLmZqj/GXR9rgeSLJf1bVG0bdx0w8HPRCa4GbgD8H/rSq9ib5aU//+A94xalDQFX1ZJJrgC8meSNT\n4diT54HXA09Nqy9t03pyoqpOAj9J8h9VdRygqn6apKttkeS7Z5sELBlmL/NlCExTVc8Dn07yD+35\nGP1up2NJ1lTVXoC2R/C7wF3A20bb2tB9FNiT5HHg6VZ7A/CLwIfPutTL08+S/FxV/QS46lQxyWvp\nLxCXANcC/z2tHuBfh9/O3PX6j9uMquoI8AdJ3gscH3U/I7IJODFYqKoTwKYkfz2alkajqr6a5M1M\n/U2MwRPDD7dvxT35jap6Dv7/S9MprwQ2j6alkflH4NWnvigNSvLPw29n7jwnIEkd8z4BSeqYISBJ\nHTMEJKljhoAkdez/AGhX6eqOqoXlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a7f8eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_noerror.category.value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label_train, feature_vector_valid, label_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label_train)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return {'accuracy': accuracy_score(label_valid, predictions), \n",
    "            # 'f2_seperate': fbeta_score(Y_test, Y_predict, average=None, beta=2),\n",
    "            'f2_macro': fbeta_score(label_valid, predictions, average='macro', beta=2),\n",
    "            'f2_micro': fbeta_score(label_valid, predictions, average='micro', beta=2),\n",
    "            'f2_weighted': fbeta_score(label_valid, predictions, average='weighted', beta=2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_data(train_data):\n",
    "    X = train_data[\"data\"].values\n",
    "    y = train_data[\"category\"].values\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        yield X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',\n",
    "                                   # token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(1,3), \n",
    "                                   max_df=1.0,\n",
    "                                   min_df=1,\n",
    "                                   max_features=8000)\n",
    "\n",
    "tfidf_vect_ngram.fit(df_noerror[\"data\"].tolist() + df_test[\"data\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1781,)\n",
      "(1792,)\n",
      "(1814,)\n",
      "(1794,)\n",
      "(1694,)\n",
      "(1786,)\n",
      "(1806,)\n",
      "(1770,)\n",
      "(1742,)\n",
      "(1759,)\n",
      "mean accuracy: 0.6632669983416253\n",
      "mean f2_macro: 0.5600644522416918\n",
      "mean f2_micro: 0.6632669983416253\n",
      "mean f2_weighted: 0.653240246122196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "    clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, n_estimators=140)\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    xsemi_tfidf_ngram = tfidf_vect_ngram.transform(np.concatenate([X_test, df_test[\"data\"]]))\n",
    "\n",
    "    # First training using only training set\n",
    "    _ = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "\n",
    "    # semi supervised stage\n",
    "    for epoch in range(epochs):\n",
    "        # create labels\n",
    "        prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "        THRESHOLD = 0.8\n",
    "        mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "        ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "        print(ysemi[mask].shape)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            # Use SVC for last epoch\n",
    "            metrics = train_model(SVC(kernel='linear'),\n",
    "                                  sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train, ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "#             metrics = train_model(xgboost.XGBClassifier(max_depth=5, n_estimators=140),\n",
    "#                               sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "#                               np.concatenate((Y_train, ysemi[mask])), \n",
    "#                               xvalid_tfidf_ngram, Y_test)\n",
    "        else:\n",
    "            metrics = train_model(clf,\n",
    "                                  sp.vstack((xtrain_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                                  np.concatenate((Y_train, ysemi[mask])), \n",
    "                                  xvalid_tfidf_ngram, Y_test)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# warn, epoch=1\n",
    "\n",
    "# RF + SVC, 0.6\n",
    "# 0.6749  # however does NOT improve final submission\n",
    "\n",
    "# RF + SVC, 0.7\n",
    "# 0.6753\n",
    "\n",
    "# RF + SVC, 0.8\n",
    "# 0.6744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6304347826086957, 'f2_macro': 0.46287850118195684, 'f2_micro': 0.6304347826086957, 'f2_weighted': 0.6160047086102571}\n",
      "{'accuracy': 0.677536231884058, 'f2_macro': 0.5993037343164005, 'f2_micro': 0.677536231884058, 'f2_weighted': 0.6707859530901795}\n",
      "{'accuracy': 0.644927536231884, 'f2_macro': 0.5323831327174694, 'f2_micro': 0.644927536231884, 'f2_weighted': 0.6377515078910662}\n",
      "{'accuracy': 0.6666666666666666, 'f2_macro': 0.5351557098422033, 'f2_micro': 0.6666666666666666, 'f2_weighted': 0.6554856094981947}\n",
      "{'accuracy': 0.644927536231884, 'f2_macro': 0.5331352327226447, 'f2_micro': 0.644927536231884, 'f2_weighted': 0.6334012756125253}\n",
      "mean accuracy: 0.6528985507246376\n",
      "mean f2_macro: 0.5325712621561349\n",
      "mean f2_micro: 0.6528985507246376\n",
      "mean f2_weighted: 0.6426858109404445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a single classifier\n",
    "metrics_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(df_noerror):\n",
    "#     clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "#     clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "#     clf = xgboost.XGBClassifier(max_depth=5, n_estimators=140)\n",
    "    clf = SVC(kernel='linear')\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    metrics = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "    \n",
    "    print(metrics)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warn\n",
    "\n",
    "SVC\n",
    "0.6730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462,)\n",
      "On training set: \n",
      "mean accuracy: 0.7696220930232558\n",
      "mean f2_macro: 0.6551242627384055\n",
      "mean f2_micro: 0.7696220930232558\n",
      "mean f2_weighted: 0.7614118872916578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION\n",
    "\n",
    "# Use only high confident data as extra training data\n",
    "metrics_list = []\n",
    "\n",
    "# clf = LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')\n",
    "clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"data\"])\n",
    "xsemi_tfidf_ngram = tfidf_vect_ngram.transform(df_test[\"data\"])\n",
    "\n",
    "# First training using only training set\n",
    "_ = train_model(clf, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "\n",
    "# semi supervised stage\n",
    "for epoch in range(epochs):\n",
    "    # create labels\n",
    "    prob_semi = clf.predict_proba(xsemi_tfidf_ngram)\n",
    "    THRESHOLD = 0.6\n",
    "    mask = np.amax(prob_semi, axis=1) > THRESHOLD\n",
    "\n",
    "    ysemi = clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "    print(ysemi[mask].shape)\n",
    "\n",
    "    if epoch == epochs-1:\n",
    "        # Use SVC for last epoch\n",
    "        final_clf = SVC(kernel='linear')\n",
    "        metrics = train_model(final_clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "    else:\n",
    "        metrics = train_model(clf,\n",
    "                          sp.vstack((train_tfidf_ngram, xsemi_tfidf_ngram[mask])),\n",
    "                          np.concatenate((df_noerror[\"category\"], ysemi[mask])), \n",
    "                          train_tfidf_ngram, df_noerror[\"category\"])\n",
    "metrics_list.append(metrics)\n",
    "\n",
    "print(\"On training set: \")\n",
    "print(\"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "print(\"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "print(\"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test[\"category\"] = final_clf.predict(xsemi_tfidf_ngram)\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors:  0.6826462128475551\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(LogisticRegression(penalty='l2', C=1.25, solver='lbfgs'), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors:  0.6586768935762224\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(MultinomialNB(alpha=0.8), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.6711409395973155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=160), xtrain_tfidf_ngram.tocsc(), Y_train, xvalid_tfidf_ngram.tocsc(), Y_test)\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors:  0.653910149750416\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(RandomForestClassifier(n_estimators=200, criterion='entropy'), xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "print(\"RF, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems that by filtering out those in the train_warn_list, the accuracy obtained is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission Using Different Mothods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=3000)\n",
    "tfidf_vect_ngram.fit(df_noerror[\"data\"])\n",
    "\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(df_noerror[\"data\"])\n",
    "test_tfidf_ngram =  tfidf_vect_ngram.transform(df_test[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR on training set  0.7286509307234695\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(penalty='l2', C=1.25, solver='lbfgs')\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"LR on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier on Ngram Level TF IDF Vectors\n",
    "classifier =  MultinomialNB()\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"NB on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB on training set  0.8842832469775475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "classifier = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"XGB on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF on training set  0.998272884283247\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "accuracy = train_model(classifier, train_tfidf_ngram, df_noerror[\"category\"], train_tfidf_ngram, df_noerror[\"category\"])\n",
    "print(\"RF on training set \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_data[\"category\"] = classifier.predict(test_tfidf_ngram.tocsc())\n",
    "df_test[\"category\"] = classifier.predict(test_tfidf_ngram)\n",
    "\n",
    "out = pd.DataFrame(df_test,columns=['article_id','category'])\n",
    "out.to_csv('prediction_fulltext.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update prediction by title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('prediction.csv')\n",
    "df_pred_fulltext = pd.read_csv('prediction_fulltext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updated_id = []\n",
    "updated_cat = []\n",
    "num_diff = 0\n",
    "for row in df_pred.itertuples():\n",
    "    updated_id.append(row.article_id)\n",
    "    cat_1 = row.category\n",
    "    _ls = df_pred_fulltext[df_pred_fulltext.article_id==row.article_id].category.values\n",
    "   \n",
    "    if _ls.size > 0:  # if calculated in fulltext prediction\n",
    "        cat_2 = _ls[0]\n",
    "        # print(cat_1, cat_2)\n",
    "        updated_cat.append(cat_2)\n",
    "        if cat_1 != cat_2:\n",
    "            num_diff += 1\n",
    "    else:\n",
    "        updated_cat.append(cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_updated = pd.DataFrame({\n",
    "    'article_id': updated_id,\n",
    "    'category': updated_cat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_updated.to_csv('prediction_updated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
