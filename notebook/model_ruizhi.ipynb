{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, fbeta_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../input/train_v2.csv\")\n",
    "test_data = pd.read_csv(\"../input/test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Forex - Pound drops to one-month lows against ...</td>\n",
       "      <td>http://www.nasdaq.com/article/forex-pound-drop...</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>www.nasdaq.com</td>\n",
       "      <td>1.390000e+12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0           1  Forex - Pound drops to one-month lows against ...   \n",
       "\n",
       "                                                 url publisher  \\\n",
       "0  http://www.nasdaq.com/article/forex-pound-drop...    NASDAQ   \n",
       "\n",
       "         hostname     timestamp  category  \n",
       "0  www.nasdaq.com  1.390000e+12         4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a238d89e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADudJREFUeJzt3X+s3XV9x/HnS1DCphgMd01tq8WsTgtudTSVxWXBmY1O\nlxWTzZQ/bLM46kJ1mJhl4P6Qf5rwx9SMRNjqZJTF2XT+CM0GGqxuxiwIF9ZY2trRCYzelHKdy6rR\n4Fre++N+up5cWu7PnlP4PB/Jyfmc9/fHeZ9voK/z/XVuqgpJUp9eMeoGJEmjYwhIUscMAUnqmCEg\nSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOnbhqBuYyWWXXVYrV64cdRuS9JLyyCOP/KCqxmaa77wP\ngZUrVzI+Pj7qNiTpJSXJU7OZz8NBktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOGgCR1zBCQpI6d\n9zeLLdTKm/9p1C0A8ORt7x11C5L0Au4JSFLHDAFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNA\nkjpmCEhSxwwBSeqYISBJHTMEJKljhoAkdWzGEEiyIsk3kxxIsj/JTa1+a5KJJHvb4z0Dy9yS5HCS\nQ0muHahflWRfm3Z7kpybjyVJmo3Z/JT0CeBjVfVoktcAjyR5oE37dFX9xeDMSVYDG4ErgNcDX0/y\n5qo6CdwJ3AB8B7gPWA/cvzgfRZI0VzPuCVTV0ap6tI1/BBwElr3IIhuAnVX1XFU9ARwG1iVZClxS\nVQ9WVQH3ANct+BNIkuZtTucEkqwE3s7UN3mAjyT5bpK7klzaasuApwcWO9Jqy9p4el2SNCKzDoEk\nrwa+BHy0qo4zdWjnTcAa4CjwycVqKsmWJONJxicnJxdrtZKkaWYVAkleyVQAfL6qvgxQVceq6mRV\nPQ98FljXZp8AVgwsvrzVJtp4ev0Fqmp7Va2tqrVjY2Nz+TySpDmYzdVBAT4HHKyqTw3Ulw7M9j7g\nsTbeDWxMclGSy4FVwENVdRQ4nuTqts5NwL2L9DkkSfMwm6uD3gl8ANiXZG+rfRy4PskaoIAngQ8B\nVNX+JLuAA0xdWbS1XRkEcCNwN3AxU1cFeWWQJI3QjCFQVd8GznQ9/30vssw2YNsZ6uPAlXNpUJJ0\n7njHsCR1zBCQpI4ZApLUMUNAkjpmCEhSxwwBSeqYISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6\nZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOG\ngCR1zBCQpI4ZApLUMUNAkjpmCEhSx2YMgSQrknwzyYEk+5Pc1OqvS/JAksfb86UDy9yS5HCSQ0mu\nHahflWRfm3Z7kpybjyVJmo3Z7AmcAD5WVauBq4GtSVYDNwN7qmoVsKe9pk3bCFwBrAfuSHJBW9ed\nwA3AqvZYv4ifRZI0RzOGQFUdrapH2/hHwEFgGbAB2NFm2wFc18YbgJ1V9VxVPQEcBtYlWQpcUlUP\nVlUB9wwsI0kagTmdE0iyEng78B1gSVUdbZOeAZa08TLg6YHFjrTasjaeXj/T+2xJMp5kfHJyci4t\nSpLmYNYhkOTVwJeAj1bV8cFp7Zt9LVZTVbW9qtZW1dqxsbHFWq0kaZpZhUCSVzIVAJ+vqi+38rF2\niIf2/GyrTwArBhZf3moTbTy9LkkakdlcHRTgc8DBqvrUwKTdwOY23gzcO1DfmOSiJJczdQL4oXbo\n6HiSq9s6Nw0sI0kagQtnMc87gQ8A+5LsbbWPA7cBu5J8EHgKeD9AVe1Psgs4wNSVRVur6mRb7kbg\nbuBi4P72kCSNyIwhUFXfBs52Pf+7z7LMNmDbGerjwJVzaVCSdO54x7AkdcwQkKSOGQKS1DFDQJI6\nZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOG\ngCR1bDZ/XlIvF7e+dtQdTLn1f0bdgaTGPQFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNAkjpm\nCEhSxwwBSeqYISBJHTMEJKljM4ZAkruSPJvksYHarUkmkuxtj/cMTLslyeEkh5JcO1C/Ksm+Nu32\nJFn8jyNJmovZ7AncDaw/Q/3TVbWmPe4DSLIa2Ahc0Za5I8kFbf47gRuAVe1xpnVKkoZoxhCoqm8B\nP5zl+jYAO6vquap6AjgMrEuyFLikqh6sqgLuAa6bb9OSpMWxkHMCH0ny3Xa46NJWWwY8PTDPkVZb\n1sbT62eUZEuS8STjk5OTC2hRkvRi5hsCdwJvAtYAR4FPLlpHQFVtr6q1VbV2bGxsMVctSRowrxCo\nqmNVdbKqngc+C6xrkyaAFQOzLm+1iTaeXpckjdC8QqAd4z/lfcCpK4d2AxuTXJTkcqZOAD9UVUeB\n40mublcFbQLuXUDfkqRFMOOfl0zyBeAa4LIkR4BPANckWQMU8CTwIYCq2p9kF3AAOAFsraqTbVU3\nMnWl0cXA/e0hSRqhGUOgqq4/Q/lzLzL/NmDbGerjwJVz6k6SdE55x7AkdcwQkKSOGQKS1DFDQJI6\nZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWOG\ngCR1zBCQpI4ZApLUMUNAkjpmCEhSxwwBSeqYISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghI\nUsdmDIEkdyV5NsljA7XXJXkgyePt+dKBabckOZzkUJJrB+pXJdnXpt2eJIv/cSRJczGbPYG7gfXT\najcDe6pqFbCnvSbJamAjcEVb5o4kF7Rl7gRuAFa1x/R1SpKGbMYQqKpvAT+cVt4A7GjjHcB1A/Wd\nVfVcVT0BHAbWJVkKXFJVD1ZVAfcMLCNJGpH5nhNYUlVH2/gZYEkbLwOeHpjvSKsta+Pp9TNKsiXJ\neJLxycnJebYoSZrJgk8Mt2/2tQi9DK5ze1Wtraq1Y2Nji7lqSdKA+YbAsXaIh/b8bKtPACsG5lve\nahNtPL0uSRqh+YbAbmBzG28G7h2ob0xyUZLLmToB/FA7dHQ8ydXtqqBNA8tIkkbkwplmSPIF4Brg\nsiRHgE8AtwG7knwQeAp4P0BV7U+yCzgAnAC2VtXJtqobmbrS6GLg/vaQJI3QjCFQVdefZdK7zzL/\nNmDbGerjwJVz6k6SdE55x7AkdcwQkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR0zBCSpY4aAJHXM\nEJCkjs34sxHSy9Hbdrxt1C0AsG/zvlG3oM65JyBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghI\nUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdMwQkqWMLCoEk\nTybZl2RvkvFWe12SB5I83p4vHZj/liSHkxxKcu1Cm5ckLcxi7Am8q6rWVNXa9vpmYE9VrQL2tNck\nWQ1sBK4A1gN3JLlgEd5fkjRP5+Jw0AZgRxvvAK4bqO+squeq6gngMLDuHLy/JGmWFhoCBXw9ySNJ\ntrTakqo62sbPAEvaeBnw9MCyR1rtBZJsSTKeZHxycnKBLUqSzubCBS7/61U1keQXgAeSfG9wYlVV\nkprrSqtqO7AdYO3atXNeXpI0OwvaE6iqifb8LPAVpg7vHEuyFKA9P9tmnwBWDCy+vNUkSSMy7xBI\n8vNJXnNqDPw28BiwG9jcZtsM3NvGu4GNSS5KcjmwCnhovu8vSVq4hRwOWgJ8Jcmp9fx9VX01ycPA\nriQfBJ4C3g9QVfuT7AIOACeArVV1ckHdS1qwg29566hbAOCt3zs46ha6NO8QqKrvA79yhvp/Ae8+\nyzLbgG3zfU9J0uLyjmFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUsYX+bIQkvWx85o+/MeoWANj6\nV785tPdyT0CSOmYISFLHDAFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNAkjpmCEhSxwwBSeqY\nISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR0zBCSpY4aAJHVs6CGQZH2S\nQ0kOJ7l52O8vSTptqCGQ5ALgM8DvAKuB65OsHmYPkqTThr0nsA44XFXfr6qfATuBDUPuQZLUpKqG\n92bJ7wPrq+qP2usPAO+oqg9Pm28LsKW9/CXg0NCaPLPLgB+MuIfzhdviNLfFaW6L086XbfHGqhqb\naaYLh9HJXFXVdmD7qPs4Jcl4Va0ddR/nA7fFaW6L09wWp73UtsWwDwdNACsGXi9vNUnSCAw7BB4G\nViW5PMmrgI3A7iH3IElqhno4qKpOJPkw8DXgAuCuqto/zB7m6bw5NHUecFuc5rY4zW1x2ktqWwz1\nxLAk6fziHcOS1DFDQJI6ZghIUscMAUnq2Hl5s9j5Jsk9VbVp1H2MQpK3AMuA71TVjwfq66vqq6Pr\nbPjattjA1PaAqXtcdlfVwdF1NRpJ1gFVVQ+33/9aD3yvqu4bcWuaI68OmibJ9PsWArwL+AZAVf3e\n0JsakSR/AmwFDgJrgJuq6t427dGq+tVR9jdMSf4MuJ6p37s60srLmbrXZWdV3Taq3oYtySeY+hHI\nC4EHgHcA3wR+C/haVW0bYXvnjSR/WFV/O+o+ZmIITJPkUeAA8DdAMRUCX2Dqf3aq6l9G191wJdkH\n/FpV/TjJSuCLwN9V1V8m+beqevtIGxyiJP8OXFFV/zut/ipgf1WtGk1nw9f+u1gDXAQ8AyyvquNJ\nLmZqj/GXR9rgeSLJf1bVG0bdx0w8HPRCa4GbgD8H/rSq9ib5aU//+A94xalDQFX1ZJJrgC8meSNT\n4diT54HXA09Nqy9t03pyoqpOAj9J8h9VdRygqn6apKttkeS7Z5sELBlmL/NlCExTVc8Dn07yD+35\nGP1up2NJ1lTVXoC2R/C7wF3A20bb2tB9FNiT5HHg6VZ7A/CLwIfPutTL08+S/FxV/QS46lQxyWvp\nLxCXANcC/z2tHuBfh9/O3PX6j9uMquoI8AdJ3gscH3U/I7IJODFYqKoTwKYkfz2alkajqr6a5M1M\n/U2MwRPDD7dvxT35jap6Dv7/S9MprwQ2j6alkflH4NWnvigNSvLPw29n7jwnIEkd8z4BSeqYISBJ\nHTMEJKljhoAkdez/AGhX6eqOqoXlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2394b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.category.value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(sent, stemmer_type='porter'):\n",
    "    '''\n",
    "    stemmer_type can be porter, lancaster, or snowball\n",
    "    '''\n",
    "    if stemmer_type == 'porter':\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer_type == 'lancaster':\n",
    "        stemmer = LancasterStemmer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    words = gensim.utils.simple_preprocess(sent)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    words = gensim.utils.simple_preprocess(sent)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hertz to Exit Equipment Rental Business in $2.5B Spinoff'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"title\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hertz to exit equip rental busi in spinoff'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"title\"].apply(stem)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform stemming\n",
    "train_data.loc[:, \"title\"] = train_data[\"title\"].apply(stem)\n",
    "test_data.loc[:, \"title\"] = test_data[\"title\"].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(train_data[\"title\"], train_data[\"category\"], test_size = 0.2)\n",
    "# X_train = np.array(X_train)\n",
    "# X_test = np.array(X_test)  # indeed X_valid is a more accurate name\n",
    "# Y_train = np.array(Y_train)\n",
    "# Y_test = np.array(Y_test)  # indeed Y_valid is a more accurate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label_train, feature_vector_valid, label_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label_train)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return {'accuracy': accuracy_score(label_valid, predictions), \n",
    "            # 'f2_seperate': fbeta_score(Y_test, Y_predict, average=None, beta=2),\n",
    "            'f2_macro': fbeta_score(label_valid, predictions, average='macro', beta=2),\n",
    "            'f2_micro': fbeta_score(label_valid, predictions, average='micro', beta=2),\n",
    "            'f2_weighted': fbeta_score(label_valid, predictions, average='weighted', beta=2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_data(train_data):\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in sss.split(train_data[\"title\"], train_data[\"category\"]):\n",
    "        X_train, X_test = train_data[\"title\"][train_index], train_data[\"title\"][test_index]\n",
    "        Y_train, Y_test = train_data[\"category\"][train_index], train_data[\"category\"][test_index]\n",
    "        yield X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Not performing well\n",
    "# def random_sample(X, y, method='RandomOverSampler'):\n",
    "#     if method == 'RandomOverSampler':\n",
    "#         sampler = RandomOverSampler()\n",
    "#     elif method == 'RandomUnderSampler':\n",
    "#         sampler = RandomUnderSampler()\n",
    "#     elif method == 'ADASYN':\n",
    "#         sampler = ADASYN()\n",
    "#     elif method == 'SMOTE':\n",
    "#         sampler = SMOTE()\n",
    "\n",
    "#     return sampler.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# le_hostname = preprocessing.LabelEncoder()\n",
    "# le_hostname.fit_transform(train_data['hostname']).shape\n",
    "\n",
    "# le_publisher = preprocessing.LabelEncoder()\n",
    "# le_publisher.fit_transform(train_data['publisher'].fillna('UNKNOWN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word',\n",
    "                             ngram_range=(1,2),\n",
    "                             # token_pattern=r'\\w{1,}',\n",
    "                             min_df=1,\n",
    "                             max_features=4000)\n",
    "count_vect.fit(train_data[\"title\"].tolist() + test_data[\"title\"].tolist())\n",
    "# count_vect.fit(train_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/ruizhi/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR mean accuracy: 0.6427031509121062\n",
      "LR mean f2_macro: 0.5571113588106928\n",
      "LR mean f2_micro: 0.6427031509121062\n",
      "LR mean f2_weighted: 0.6374668341887142\n",
      "\n",
      "NB mean accuracy: 0.6372305140961857\n",
      "NB mean f2_macro: 0.5792688637243283\n",
      "NB mean f2_micro: 0.6372305140961857\n",
      "NB mean f2_weighted: 0.635536090627901\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7d46be775dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mxvalid_count\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclf_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'XGB'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-165d5ed89ddf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label_train, feature_vector_valid, label_valid)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# predict the labels on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    545\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1021\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for clf_name, clf in [('LR', LogisticRegression()),\n",
    "                      ('NB', MultinomialNB()),\n",
    "                      ('XGB', xgboost.XGBClassifier(max_depth=5, n_estimators=140)),\n",
    "                      ('RF', RandomForestClassifier(n_estimators=180, criterion='entropy'))]:\n",
    "    metrics_list = []\n",
    "    for X_train, Y_train, X_test, Y_test in cross_validation_data(train_data):\n",
    "        xtrain_count =  count_vect.transform(X_train)\n",
    "        xvalid_count =  count_vect.transform(X_test)\n",
    "        if clf_name == 'XGB':\n",
    "            metrics = train_model(clf, xtrain_count.tocsc(), Y_train, xvalid_count.tocsc(), Y_test)\n",
    "        else:\n",
    "            metrics = train_model(clf, xtrain_count, Y_train, xvalid_count, Y_test)\n",
    "        metrics_list.append(metrics)\n",
    "    print(clf_name, \"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "    print(clf_name, \"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "    print(clf_name, \"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "    print(clf_name, \"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "# accuracy = train_model(clf, xtrain_count, Y_train, xvalid_count, Y_test)\n",
    "\n",
    "# Y_predict = clf.predict(xvalid_count)\n",
    "# print(fbeta_score(Y_test, Y_predict, average=None, beta=1))\n",
    "# print(fbeta_score(Y_test, Y_predict, average='macro', beta=1))\n",
    "# print(fbeta_score(Y_test, Y_predict, average='weighted', beta=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',\n",
    "                                   # token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(1,2), \n",
    "                                   max_df=1.0,\n",
    "                                   min_df=1,\n",
    "                                   max_features=4000)\n",
    "tfidf_vect_ngram.fit(train_data[\"title\"].tolist() + test_data[\"title\"].tolist())\n",
    "# tfidf_vect_ngram.fit(train_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR mean accuracy: 0.6563847429519072\n",
      "LR mean f2_macro: 0.5478208767241433\n",
      "LR mean f2_micro: 0.6563847429519072\n",
      "LR mean f2_weighted: 0.6471283940542423\n",
      "\n",
      "NB mean accuracy: 0.6540630182421228\n",
      "NB mean f2_macro: 0.5217897243700211\n",
      "NB mean f2_micro: 0.6540630182421228\n",
      "NB mean f2_weighted: 0.641110679139451\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-53b921d0bf19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mxvalid_tfidf_ngram\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtfidf_vect_ngram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclf_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'XGB'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_tfidf_ngram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_tfidf_ngram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_tfidf_ngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_tfidf_ngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-165d5ed89ddf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label_train, feature_vector_valid, label_valid)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# predict the labels on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    545\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruizhi/anaconda/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1021\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for clf_name, clf in [('LR', LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')),\n",
    "                      ('NB', MultinomialNB()),\n",
    "                      ('XGB', xgboost.XGBClassifier(max_depth=5, n_estimators=140)),\n",
    "                      ('RF', RandomForestClassifier(n_estimators=180, criterion='entropy'))]:\n",
    "    metrics_list = []\n",
    "    for X_train, Y_train, X_test, Y_test in cross_validation_data(train_data):\n",
    "        xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "        xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "        if clf_name == 'XGB':\n",
    "            metrics = train_model(clf, xtrain_tfidf_ngram.tocsc(), Y_train, xvalid_tfidf_ngram.tocsc(), Y_test)\n",
    "        else:\n",
    "            metrics = train_model(clf, xtrain_tfidf_ngram, Y_train, xvalid_tfidf_ngram, Y_test)\n",
    "        metrics_list.append(metrics)\n",
    "    print(clf_name, \"mean accuracy:\", sum([item['accuracy'] for item in metrics_list])/len(metrics_list))\n",
    "    print(clf_name, \"mean f2_macro:\", sum([item['f2_macro'] for item in metrics_list])/len(metrics_list))    \n",
    "    print(clf_name, \"mean f2_micro:\", sum([item['f2_micro'] for item in metrics_list])/len(metrics_list))    \n",
    "    print(clf_name, \"mean f2_weighted:\", sum([item['f2_weighted'] for item in metrics_list])/len(metrics_list))  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain_tfidf_resampled, Y_train_resampled = random_sample(xtrain_tfidf_ngram, Y_train, method='ADASYN')\n",
    "\n",
    "# # Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "# accuracy = train_model(xgboost.XGBClassifier(max_depth=5, n_estimators=140), xtrain_tfidf_resampled.tocsc(), Y_train_resampled, xvalid_tfidf_ngram.tocsc(), Y_test)\n",
    "# print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = xgboost.XGBClassifier(max_depth=5, n_estimators=140)\n",
    "# accuracy = train_model(clf, xtrain_tfidf_resampled.tocsc(), Y_train_resampled, xvalid_tfidf_ngram.tocsc(), Y_test)\n",
    "\n",
    "# Y_predict = clf.predict(xvalid_tfidf_ngram.tocsc())\n",
    "# print(fbeta_score(Y_test, Y_predict, average=None, beta=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuse_results(predictions_list, xgb_offset=1):\n",
    "    final_predictions = []\n",
    "    percentage = 0\n",
    "    for preds in zip(*predictions_list):\n",
    "        sr_count = pd.Series(preds).value_counts()\n",
    "        if sr_count.iloc[0] >= 2 and sr_count.index[0] != preds[1]:\n",
    "            final_predictions.append(sr_count.index[0])  # the most commonly predicted class\n",
    "            percentage +=1\n",
    "        else:\n",
    "            final_predictions.append(preds[1])  # use result of xgb\n",
    "    \n",
    "    print(percentage)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "0.6575456053067993\n",
      "43\n",
      "0.6558872305140961\n",
      "56\n",
      "0.6716417910447762\n",
      "56\n",
      "0.6658374792703151\n",
      "58\n",
      "0.6749585406301825\n",
      "50\n",
      "0.664179104477612\n",
      "59\n",
      "0.6650082918739635\n",
      "51\n",
      "0.6890547263681592\n",
      "56\n",
      "0.693200663349917\n",
      "72\n",
      "0.6509121061359867\n",
      "0.6688225538971808\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',\n",
    "                                   # token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(1,2), \n",
    "                                   max_df=1.0,\n",
    "                                   min_df=2,\n",
    "                                   max_features=4000)\n",
    "tfidf_vect_ngram.fit(train_data[\"title\"].tolist() + test_data[\"title\"].tolist())\n",
    "# tfidf_vect_ngram.fit(train_data[\"title\"])\n",
    "\n",
    "\n",
    "acc_list = []\n",
    "for X_train, Y_train, X_test, Y_test in cross_validation_data(train_data):\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "    \n",
    "    predictions_list = []\n",
    "    for clf_name, clf in [\n",
    "                          ('KNN', KNeighborsClassifier(n_neighbors=40, weights='distance')),\n",
    "#                           ('LR', LogisticRegression()),\n",
    "#                           ('LR', LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')),\n",
    "                          ('NB', MultinomialNB(alpha=0.5, fit_prior=True)),\n",
    "#                           ('RF', RandomForestClassifier(n_estimators=180, criterion='entropy')),\n",
    "                          ('XGB', xgboost.XGBClassifier(max_depth=5, n_estimators=140)),\n",
    "#                           ('RF', RandomForestClassifier(n_estimators=180, criterion='entropy'))\n",
    "                         ]:\n",
    "        if clf_name == 'XGB':\n",
    "            clf.fit(xtrain_tfidf_ngram.tocsc(), Y_train)\n",
    "            predictions = clf.predict(xvalid_tfidf_ngram.tocsc())\n",
    "        else:\n",
    "            clf.fit(xtrain_tfidf_ngram, Y_train)\n",
    "            predictions = clf.predict(xvalid_tfidf_ngram)\n",
    "    \n",
    "        predictions_list.append(predictions)\n",
    "    \n",
    "    final_predictions = fuse_results(predictions_list)\n",
    "    accuracy = accuracy_score(Y_test, final_predictions)\n",
    "    print(accuracy)\n",
    "    acc_list.append(accuracy)\n",
    "\n",
    "print(sum(acc_list)/len(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lr, rf, xgb\n",
    "# 0.6624378109452737\n",
    "\n",
    "# lr_ovr, nb, xgb\n",
    "# 0.6604477611940299\n",
    "\n",
    "# knn, lr, xgb\n",
    "# 0.6640132669983416\n",
    "\n",
    "# knn, nb, xgb\n",
    "# 0.6688225538971808\n",
    "\n",
    "# knn, rf, xgb\n",
    "# 0.6671641791044776\n",
    "\n",
    "# nb, rf, xgb\n",
    "# 0.6629353233830845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "# create submission\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(train_data[\"title\"])\n",
    "test_tfidf_ngram =  tfidf_vect_ngram.transform(test_data[\"title\"])\n",
    "\n",
    "    \n",
    "predictions_list = []\n",
    "for clf_name, clf in [\n",
    "                      ('KNN', KNeighborsClassifier(n_neighbors=40, weights='distance')),\n",
    "#                       ('LR', LogisticRegression()),\n",
    "#                       ('LR', LogisticRegression(penalty='l2', C=2, solver='liblinear', multi_class='ovr')),\n",
    "                      ('NB', MultinomialNB(alpha=0.5, fit_prior=True)),\n",
    "#                       ('RF', RandomForestClassifier(n_estimators=180, criterion='entropy')),\n",
    "                      ('XGB', xgboost.XGBClassifier(max_depth=5, n_estimators=140)),\n",
    "#                       ('RF', RandomForestClassifier(n_estimators=180, criterion='entropy'))\n",
    "                     ]:\n",
    "    if clf_name == 'XGB':\n",
    "        clf.fit(train_tfidf_ngram.tocsc(), train_data[\"category\"])\n",
    "        predictions = clf.predict(test_tfidf_ngram.tocsc())\n",
    "    else:\n",
    "        clf.fit(train_tfidf_ngram, train_data[\"category\"])\n",
    "        predictions = clf.predict(test_tfidf_ngram)\n",
    "\n",
    "    predictions_list.append(predictions)\n",
    "\n",
    "final_predictions = fuse_results(predictions_list)\n",
    "\n",
    "test_data[\"category\"] = final_predictions\n",
    "\n",
    "out=pd.DataFrame(test_data,columns=['article_id','category'])\n",
    "out.to_csv('prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(train_data[\"title\"])\n",
    "test_tfidf_ngram =  tfidf_vect_ngram.transform(test_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB on training set  0.7375145180023229\n"
     ]
    }
   ],
   "source": [
    "classifier = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=140)\n",
    "metrics = train_model(classifier, train_tfidf_ngram.tocsc(), train_data[\"category\"], train_tfidf_ngram.tocsc(), train_data[\"category\"])\n",
    "print(\"XGB on training set \", metrics)\n",
    "\n",
    "test_data[\"category\"] = classifier.predict(test_tfidf_ngram.tocsc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB on training set  {'accuracy': 0.9905425584868094, 'f2_macro': 0.9906536277966065, 'f2_micro': 0.9905425584868094, 'f2_weighted': 0.9905380591051429}\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=40, weights='distance')\n",
    "metrics = train_model(classifier, train_tfidf_ngram, train_data[\"category\"], train_tfidf_ngram, train_data[\"category\"])\n",
    "print(\"XGB on training set \", metrics)\n",
    "\n",
    "test_data[\"category\"] = classifier.predict(test_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators=180, criterion='entropy')\n",
    "# metrics = train_model(classifier, train_tfidf_ngram, train_data[\"category\"], train_tfidf_ngram, train_data[\"category\"])\n",
    "# print(\"RF on training set \", metrics)\n",
    "\n",
    "# test_data[\"category\"] = classifier.predict(test_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out=pd.DataFrame(test_data,columns=['article_id','category'])\n",
    "out.to_csv('prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
